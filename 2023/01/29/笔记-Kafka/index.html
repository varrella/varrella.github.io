<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Kafka笔记 | varrella</title><meta name="author" content="Varrella"><meta name="copyright" content="Varrella"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一 Kafka概述Kafka是一个分布式的基于发布&#x2F; 订阅模式的消息队列，主要应用于大数据实时处理领域。 1 消息队列的好处 解耦：允许独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束； 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理； 缓冲：有助于空值和优化数据流经过系统的">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka笔记">
<meta property="og:url" content="https://varrella.github.io/2023/01/29/%E7%AC%94%E8%AE%B0-Kafka/index.html">
<meta property="og:site_name" content="varrella">
<meta property="og:description" content="一 Kafka概述Kafka是一个分布式的基于发布&#x2F; 订阅模式的消息队列，主要应用于大数据实时处理领域。 1 消息队列的好处 解耦：允许独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束； 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理； 缓冲：有助于空值和优化数据流经过系统的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturegg2880x1800-bg-d1ee002.jpg">
<meta property="article:published_time" content="2023-01-29T12:06:30.580Z">
<meta property="article:modified_time" content="2022-10-14T07:45:08.000Z">
<meta property="article:author" content="Varrella">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturegg2880x1800-bg-d1ee002.jpg"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="https://varrella.github.io/2023/01/29/%E7%AC%94%E8%AE%B0-Kafka/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-14 15:45:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture微信图片_20210411174101.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">23</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturegg2880x1800-bg-d1ee002.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">varrella</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Kafka笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-01-29T12:06:30.580Z" title="Created 2023-01-29 20:06:30">2023-01-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-10-14T07:45:08.000Z" title="Updated 2022-10-14 15:45:08">2022-10-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Kafka笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="一-Kafka概述"><a href="#一-Kafka概述" class="headerlink" title="一 Kafka概述"></a>一 Kafka概述</h2><p>Kafka是一个<strong>分布式</strong>的<strong>基于发布/ 订阅模式</strong>的消息队列，主要应用于大数据实时处理领域。</p>
<h3 id="1-消息队列的好处"><a href="#1-消息队列的好处" class="headerlink" title="1 消息队列的好处"></a>1 消息队列的好处</h3><ul>
<li>解耦：允许独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束；</li>
<li>可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理；</li>
<li>缓冲：有助于空值和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况；</li>
<li>灵活性&amp;峰值处理能力：在访问量剧增的情况下，应用仍然需要继续发挥作用但是这样的突发流量并不常见，如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够是关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃；</li>
<li>异步通信：很多时候，用户不想也不需要立即处理消息，消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它，可以在队列中放很多消息，在需要的时候再去处理它们。</li>
</ul>
<h3 id="2-消息队列的两种模式"><a href="#2-消息队列的两种模式" class="headerlink" title="2 消息队列的两种模式"></a>2 消息队列的两种模式</h3><h4 id="1）点对点模式"><a href="#1）点对点模式" class="headerlink" title="1）点对点模式"></a>1）点对点模式</h4><p>一对一，消费者主动拉取数据，消息收到后消息清除。</p>
<p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并消费消息，消息被消费以后，queue中不再有存储，所以消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20220416220338.png" style="zoom:67%;" />

<h4 id="2）发布订阅模式"><a href="#2）发布订阅模式" class="headerlink" title="2）发布订阅模式"></a>2）发布订阅模式</h4><p>一对多，消费者消费数据之后不会清除消息。</p>
<p>消息生产者将消息发布到多个topic中，同时有多个消息消费者（订阅）消费该消息，和点对点方式不同，每个消费者相互独立，发布到topic的消息会被所有订阅者消费。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20220416220353.png" style="zoom:67%;" />

<h2 id="二-Kafka系统架构"><a href="#二-Kafka系统架构" class="headerlink" title="二 Kafka系统架构"></a>二 Kafka系统架构</h2><h3 id="1-Broker"><a href="#1-Broker" class="headerlink" title="1 Broker"></a>1 Broker</h3><p><code>Kafka</code>集群包含一个或多个服务器，服务器节点称为<code>broker</code>，一个broker可以容纳多个topic。</p>
<p>Broker的工作流程：</p>
<p><img src="C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20220622211031847.png" alt="image-20220622211031847"></p>
<h3 id="2-Topic"><a href="#2-Topic" class="headerlink" title="2 Topic"></a>2 Topic</h3><p>每条发布到<code>Kafka</code>集群的消息都有一个类别，这个类别称为<code>Topic</code>，物理上不同<code>Topic</code>的消息分开存储，逻辑上一个<code>Topic</code>的消息虽然保存于一个或多个<code>broker</code>上，但用户只需指定<code>Topic</code>即可生产或消费 ，而不必关心数据存于何处。可以理解为一个队列，生产者和消费者面向的都是一个topic。</p>
<h3 id="3-Partition"><a href="#3-Partition" class="headerlink" title="3 Partition"></a>3 Partition</h3><ul>
<li>为了提高吞吐量，<code>Topic</code>中的数据分割为一个或多个<code>partition</code>。每个<code>Topic</code>至少有一个<code>partition</code>，每个partition是一个有序的队列，当生产者生产数据的时候，根据分配策略，然后将消息追加到指定的分区的末尾（队列）。</li>
<li>每条消息都有一个自增的编号，用于标识顺序，以及标识消息的偏移量</li>
<li>每个<code>partition</code>中的数据使用多个<code>segment</code>文件存储</li>
<li><strong><code>partition</code>中的数据是有顺序的</strong>，不同<code>partition</code>间的数据丢失了数据的顺序</li>
<li>如果<code>topic</code>有多个<code>partition</code>，消费数据时就不能保证数据的顺序，严格保证消息的新消费顺序的场景下，需要将<code>partition</code>数目设置为1。</li>
<li>一个分区的数据只能由一个消费者消费</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20210813205902.png"></p>
<h3 id="4-Leader"><a href="#4-Leader" class="headerlink" title="4 Leader"></a>4 Leader</h3><ul>
<li><p>每个<code>partition</code>有多个副本，其中有且仅有一个作为<code>Leader</code>，<code>Leader</code>是当前负责数据的读写的<code>partition</code></p>
</li>
<li><p>```<br>1、producer先从zookeeper的”/brokers/…/state”节点找到该partition的leader<br>2、producer将消息发送给leader<br>3、leader将消息写入本地log<br>4、followers从leader pull消息，写入本地log后leader发送ACK<br>5、leader收到所有ISR中的replica的ACK后，增加HW（high watermark，最后commit的offset）并向producer发送ACK</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 5 Follower</span><br><span class="line"></span><br><span class="line">- &#96;follower&#96;跟随&#96;leader&#96;，所有写请求都通过&#96;leader&#96;路由，数据变更会广播给所有&#96;follower&#96;，&#96;follower&#96;与&#96;leader&#96;保持数据同步。</span><br><span class="line">- 如果&#96;leader&#96;失效，则从&#96;follower&#96;中选举一个新的&#96;leader&#96;</span><br><span class="line">- 当&#96;follower&#96;挂掉、卡主或者同步太慢，&#96;leader&#96;会把这个&#96;follower&#96;从&#96;“in sync replicas”（ISR）&#96;列表中删除，重新创建一个&#96;follower&#96;。</span><br><span class="line"></span><br><span class="line">### 6 Replication</span><br><span class="line"></span><br><span class="line">- 数据会存放到&#96;topic&#96;的&#96;partition&#96;中，但是有可能分区会损坏，需要对分区的数据进行备份；</span><br><span class="line"></span><br><span class="line">- 一个topic的每个分区都有若干个副本，一个leader和若干个follower；</span><br><span class="line"></span><br><span class="line">- 将分区分为&#96;leader(1)&#96;和&#96;follower(N)&#96;</span><br><span class="line"></span><br><span class="line">  - &#96;leader&#96;负责写入和读取数据，生产者发送数据的对象，以及消费者消费数据的对象都是leader；</span><br><span class="line">  - &#96;follower&#96;只负责备份，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader；</span><br><span class="line">  - 保证了数据的一致性；</span><br><span class="line"></span><br><span class="line">- 备份数设置为 &#96;N&#96;，表示&#96;主 + 备份 &#x3D; N&#96;</span><br><span class="line"></span><br><span class="line">  - &#96;&#96;&#96;</span><br><span class="line">    kafka分配Replica的算法如下：</span><br><span class="line">    1、将所有broker（假设共有n个broker）和待分配的partition排序</span><br><span class="line">    2、将第i个partition分配到第(i mod n)个broker上</span><br><span class="line">    3、将第i个partition的第j个replica分配到第((i + j) mod n)个broker上</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="7-Producer"><a href="#7-Producer" class="headerlink" title="7 Producer"></a>7 Producer</h3><ul>
<li>生产者即数据的发布者，将消息发布到<code>Kafka</code>的<code>topic</code>中</li>
<li><code>broker</code>接收到生产者发送的消息后，<code>broker</code>将消息<strong>追加</strong>到当前用于追加数据的<code>segment</code>文件中</li>
<li>生产者发送的消息，存储到一个<code>partition</code>中，生产者也可以指定数据存储的<code>partition</code></li>
</ul>
<h3 id="8-Consumer"><a href="#8-Consumer" class="headerlink" title="8 Consumer"></a>8 Consumer</h3><ul>
<li>消费者可以从<code>broker</code>中读取数据，消费者可以消费多个<code>topic</code>中的数据</li>
</ul>
<h3 id="9-Consumer-Group"><a href="#9-Consumer-Group" class="headerlink" title="9 Consumer  Group"></a>9 Consumer  Group</h3><ul>
<li><p>配合分区的设计，提出消费者组的概念，组内每个消费者并行消费</p>
</li>
<li><p>每个<code>consumer</code>属于一个特定的<code>consumer group</code>（可为每个<code>consumer</code>指定<code>group name</code>，若不指定<code>group name</code>则属于默认的<code>group</code>）</p>
</li>
<li><p>将多个消费者集中到一起去处理某一个<code>topic</code>的数据，可以更快地提高数据的消费能力</p>
</li>
<li><p>整个消费者共享一组偏移量（防止数据被重复读取，一组的多个消费者不会消费相同的数据），因为一个<code>topic</code>有多个分区</p>
</li>
</ul>
<h3 id="10-offset偏移量"><a href="#10-offset偏移量" class="headerlink" title="10 offset偏移量"></a>10 offset偏移量</h3><ul>
<li>可唯一标识一条消息</li>
<li>偏移量决定读取数据的位置，不会有线程安全的问题，消费者通过偏移量来决定下次读取的消息</li>
<li>消息被消费之后，并不会马上删除，这样多个业务就可以重复使用<code>kafka</code>的消息</li>
<li>某一个业务也可以通过修改偏移量大到重新读取消息的目的，偏移量由用户控制</li>
<li>消息最终还是会被删除的，默认生命周期为1周</li>
</ul>
<h3 id="11-元数据"><a href="#11-元数据" class="headerlink" title="11 元数据"></a>11 元数据</h3><p>元数据是指Kafka 集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的lead巳r 副本分配在哪个节点上， follower 副本分配在哪些节点上，哪些副本在AR 、ISR 等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。</p>
<p>当客户端中没有需要使用的元数据信息时，比如没有指定的主题信息，或者超过rnetadata . rnax.age.rns 时间没有更新元数据都会引起元数据的更新操作。客户端参数rnetadata. rnax.age.rns 的默认值为300000，即5 分钟。元数据的更新操作是在客户端内部进行的，对客户端的外部使用者不可见。当需要更新元数据时，会先挑选出leastLoadedNode，然后向这个Node 发送MetadataRequest 请求来获取具体的元数据信息。这个更新操作是由Sender线程发起的， 在创建完MetadataRequest 之后同样会存入InFlightRequests ，之后的步骤就和发送消息时的类似。元数据虽然由Sender 线程负责更新，但是主线程也需要读取这些信息，这里的数据同步通过synchronized 和final 关键字来保障。</p>
<h2 id="三-Kafka生产者"><a href="#三-Kafka生产者" class="headerlink" title="三 Kafka生产者"></a>三 Kafka生产者</h2><h3 id="3-1-生产者消息发送流程"><a href="#3-1-生产者消息发送流程" class="headerlink" title="3.1 生产者消息发送流程"></a>3.1 生产者消息发送流程</h3><p>在消息发送的过程中，涉及到了两个线程：</p>
<ul>
<li><code>main</code>线程：在主线程中由KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作<br>用之后缓存到消息累加器（ RecordAccumulator ，也称为消息收集器〉中。</li>
<li><code>Sender</code>线程：<code>Sender</code>线程不断从<code>RecordAccumulator</code>中拉取消息发送到<code>Kafka Broker</code>。</li>
</ul>
<p>RecordAccumulator：主要用来缓存消息以便Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。RecordAccumulator 缓存的大小可以通过生产者客户端参数buffer. memory 配置，默认值为33554432B ，即32M。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer 的send() 方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms 的配置，此参数的默认值为60000ms，即60 秒。</p>
<p>主线程中发送过来的消息都会被迫加到RecordAccumulator 的某个双端队列（ Deque ）中，在RecordAccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch ，即Deque&lt; ProducerBatch＞。消息写入缓存时，追加到双端队列的尾部，sender线程读取消息时，从双端队列头部读取。</p>
<p>【整体流程】：</p>
<ol>
<li>首先经过拦截器对数据进行加工处理；</li>
<li>然后经过序列化器对数据进行序列化；</li>
<li>然后经过分区器，来决定将数据发送到哪个分区，一个分区会创建一个队列RecordAccumulator（内存池，整个操作在内存中完成，默认大小为32M，每一批次的数据大小为16K）；</li>
<li>sender线程对数据进行拉取，按broker节点建立请求，并存入请求队列 InFlightRequest 中，每个broker节点最多缓存5个请求，将数据发送到broker集群；</li>
<li>处理集群应答，如果数据发送成功，则生产者会删除对应的请求，并清理对应的每一个分区的数据；失败则重试，从请求中再次发送并等待应答，最大重试次数为int类型最大值；</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/image2022-10-8_12-49-15.png"/>

<p>书上的图：</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009133133.png"/>

<h3 id="3-2-生产者分区"><a href="#3-2-生产者分区" class="headerlink" title="3.2 生产者分区"></a>3.2 生产者分区</h3><h4 id="3-2-1-分区的好处"><a href="#3-2-1-分区的好处" class="headerlink" title="3.2.1 分区的好处"></a>3.2.1 分区的好处</h4><ul>
<li><p>便于合理使用存储资源，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。</p>
</li>
<li><p>提高并行度，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。</p>
</li>
</ul>
<h4 id="3-2-2-生产者发送消息的分区策略"><a href="#3-2-2-生产者发送消息的分区策略" class="headerlink" title="3.2.2 生产者发送消息的分区策略"></a>3.2.2 生产者发送消息的分区策略</h4><h5 id="1-默认的分区器DefaultPartitioner"><a href="#1-默认的分区器DefaultPartitioner" class="headerlink" title="1 默认的分区器DefaultPartitioner"></a>1 默认的分区器DefaultPartitioner</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;partition&quot;</span>, <span class="string">&quot;hello world&quot;</span>));</span><br></pre></td></tr></table></figure>

<p>在 ProducerRecord() 的构造方法中：</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009115015.png"/>

<ul>
<li>指明partition的情况下，直接将指明的值作为partition值；例如partition=0，所有数据写入分区0；</li>
<li>没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；例如：key1的hash值=5， key2的hash值=6 ，topic的partition数=2，那么key1 对应的value1写入1号分区，key2对应的value2写入0号分区。</li>
<li>既没有partition值又没有key值的情况下，Kafka采用Sticky Partition（黏性分区器），会随机选择一个分区，并尽可能一直使用该分区，待该分区的batch已满或者已完成，Kafka再随机一个分区进行使用（和上一次的分区不同）。例如：第一次随机选择0号分区，等0号分区当前批次满了（默认16k）或者linger.ms设置的时间到， Kafka再随机一个分区进行使用（如果还是0会继续随机）。</li>
</ul>
<h5 id="2-自定义分区"><a href="#2-自定义分区" class="headerlink" title="2 自定义分区"></a>2 自定义分区</h5><p>可以根据需求自定义分区器，需要自定义分区类实现Partition接口，重写partition()方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        String msgValue = value.toString();</span><br><span class="line">        <span class="keyword">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span>(msgValue.contains(<span class="string">&quot;hello&quot;</span>))&#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在生产者的配置中添加自定义分区器的配置即可使用：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br></pre></td></tr></table></figure>

<h3 id="3-3-生产者客户端使用"><a href="#3-3-生产者客户端使用" class="headerlink" title="3.3 生产者客户端使用"></a>3.3 生产者客户端使用</h3><h4 id="1-配置生产者客户端参数以及创建相应的消费者实例"><a href="#1-配置生产者客户端参数以及创建相应的消费者实例" class="headerlink" title="1 配置生产者客户端参数以及创建相应的消费者实例"></a>1 配置生产者客户端参数以及创建相应的消费者实例</h4><p>相应的参数配置：</p>
<table>
<thead>
<tr>
<th>参数配置</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>指定连接kafka集群所需的broker地址列表，格式为host1:port1,host2:port2，可以设置一个或多个地址</td>
</tr>
<tr>
<td>key.serializer</td>
<td>指定消息中key所需的序列化操作的序列化器</td>
</tr>
<tr>
<td>value.serializer</td>
<td>指定消息中value所需的序列化操作的序列化器</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop:9092&quot;</span>);</span><br><span class="line">properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br></pre></td></tr></table></figure>

<h4 id="2-创建并初始化生产者对象"><a href="#2-创建并初始化生产者对象" class="headerlink" title="2 创建并初始化生产者对象"></a>2 创建并初始化生产者对象</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br></pre></td></tr></table></figure>

<h4 id="3-发送数据到broker集群"><a href="#3-发送数据到broker集群" class="headerlink" title="3 发送数据到broker集群"></a>3 发送数据到broker集群</h4><h5 id="3-1-异步发送"><a href="#3-1-异步发送" class="headerlink" title="3.1 异步发送"></a>3.1 异步发送</h5><ul>
<li>异步发送是指将外部数据发送到队列RecordAccumulator中的过程采用异步发送，不管队列中的数据是否已经发送到kafka集群，外部数据会一直发送到队列中。</li>
<li>如果是带回调函数的异步发送，则是在send()方法中带有callback回调函数，在外部数据发送到队列中后，返回发送的主题、分区等信息。</li>
</ul>
<p>异步就是批量发送。如果设置成异步的模式，可以运行生产者以batch的形式push数据，这样会极大的提高broker的性能，但是这样会增加丢失数据的风险。异步方式，可以发送一条，也可以批量发送多条，特性是不需等第一次(注意这里单位是次，因为单次可以是单条，也可以是批量数据)响应，就立即发送第二次。生产者发消息，发送完之后不用等待broker给回复，直接执行下面的业务逻辑。可以提供回调方法，让broker异步的调用callback，告知生产者，消息发送的结果。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009120744.png"/>

<h5 id="3-2-同步发送"><a href="#3-2-同步发送" class="headerlink" title="3.2 同步发送"></a>3.2 同步发送</h5><p>同步发送是指将外部数据发送到队列RecordAccumulator中的过程采用同步发送，只有第一批发送到队列中的数据，成功发送到kafka集群后，第二批数据才会进入队列，进行发送流程。</p>
<p>同步就是逐条发送。用户线程选择同步，效果是逐条发送，因为请求队列InFlightRequest中永远最多有一条数据。异步+设置 后台线程的异步发送参数：max.in.flight.requests.per.connection=1 &amp; batch.size=1，效果也是逐条发送。一定是逐条发送的，第一条响应到达后，才会请求第二条。生产者同步发消息，在收到kafka的ack告知发送成功之前一直处于阻塞状态。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009120837.png"/>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;<span class="number">10</span>; ++i) &#123;</span><br><span class="line">    <span class="comment">// 异步发送，默认</span></span><br><span class="line">    kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;hello&quot;</span> + i));</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// 同步发送</span></span><br><span class="line">    kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;hello&quot;</span> + i)).get();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-3-发送条件"><a href="#3-3-发送条件" class="headerlink" title="3.3 发送条件"></a>3.3 发送条件</h5><p>发送消息失败会默认自动重试三次，每次间隔100ms。发送的消息会先进入到本地缓冲区（32mb），kakfa会跑一个线程，该线程去缓冲区中取16k的数据，发送到kafka，如果到 10 毫秒数据没取满16k，也会发送一次。异步的时候假如设置了缓存消息数量为200，但是一直没有200条数据，那么不可能会一直等下去，就会取16kb大小的数据，直接发，不够16kb也会发。</p>
<h4 id="4-关闭资源"><a href="#4-关闭资源" class="headerlink" title="4 关闭资源"></a>4 关闭资源</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafkaProducer.close();</span><br></pre></td></tr></table></figure>

<h3 id="3-4-生产者发送消息核心流程-amp-源码"><a href="#3-4-生产者发送消息核心流程-amp-源码" class="headerlink" title="3.4 生产者发送消息核心流程 &amp; 源码"></a>3.4 生产者发送消息核心流程 &amp; 源码</h3><h4 id="3-4-1-生产者main线程和sender线程初始化"><a href="#3-4-1-生产者main线程和sender线程初始化" class="headerlink" title="3.4.1 生产者main线程和sender线程初始化"></a>3.4.1 生产者main线程和sender线程初始化</h4><h5 id="1-KafkaProducer-构造方法流程-amp-源码"><a href="#1-KafkaProducer-构造方法流程-amp-源码" class="headerlink" title="1 KafkaProducer 构造方法流程 &amp; 源码"></a>1 KafkaProducer 构造方法流程 &amp; 源码</h5><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009121039.png"/>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">KafkaProducer(ProducerConfig config,</span><br><span class="line">              Serializer&lt;K&gt; keySerializer,</span><br><span class="line">              Serializer&lt;V&gt; valueSerializer,</span><br><span class="line">              ProducerMetadata metadata,</span><br><span class="line">              KafkaClient kafkaClient,</span><br><span class="line">              ProducerInterceptors&lt;K, V&gt; interceptors,</span><br><span class="line">              Time time) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// 分区器配置（分区策略等）</span></span><br><span class="line">        <span class="keyword">this</span>.partitioner = config.getConfiguredInstance(</span><br><span class="line">            ProducerConfig.PARTITIONER_CLASS_CONFIG,</span><br><span class="line">            Partitioner.class,</span><br><span class="line">            Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// 拦截器配置，可以配置多个拦截器</span></span><br><span class="line">        List&lt;ProducerInterceptor&lt;K, V&gt;&gt; interceptorList = (List) config.getConfiguredInstances(</span><br><span class="line">            ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">            ProducerInterceptor.class,</span><br><span class="line">            Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));</span><br><span class="line">        <span class="keyword">if</span> (interceptors != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">this</span>.interceptors = interceptors;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">this</span>.interceptors = <span class="keyword">new</span> ProducerInterceptors&lt;&gt;(interceptorList);</span><br><span class="line">        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer,</span><br><span class="line">                                                                                              valueSerializer, interceptorList, reporters);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">/** 缓冲区RecordAccumulator队列配置</span></span><br><span class="line"><span class="comment">             * 批次大下，默认 16k</span></span><br><span class="line"><span class="comment">             * 是否压缩，默认 none</span></span><br><span class="line"><span class="comment">             * linger.ms ，默认值 0 。</span></span><br><span class="line"><span class="comment">             * 内存池，默认32M，每个batch默认16K</span></span><br><span class="line"><span class="comment">             * 重试间隔时间，默认值 100ms 。</span></span><br><span class="line"><span class="comment">             * delivery.timeout.ms 默认值 2 分钟。</span></span><br><span class="line"><span class="comment">             * request.timeout.ms 默 认值 30s 。</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">        <span class="keyword">this</span>.accumulator = <span class="keyword">new</span> RecordAccumulator(logContext,</span><br><span class="line">                                                 config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),</span><br><span class="line">                                                 <span class="keyword">this</span>.compressionType,</span><br><span class="line">                                                 lingerMs(config),</span><br><span class="line">                                                 retryBackoffMs,</span><br><span class="line">                                                 deliveryTimeoutMs,</span><br><span class="line">                                                 metrics,</span><br><span class="line">                                                 PRODUCER_METRIC_GROUP_NAME,</span><br><span class="line">                                                 time,</span><br><span class="line">                                                 apiVersions,</span><br><span class="line">                                                 transactionManager,</span><br><span class="line">                                                 <span class="keyword">new</span> BufferPool(<span class="keyword">this</span>.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连接kafka集群</span></span><br><span class="line">        List&lt;InetSocketAddress&gt; addresses = ClientUtils.parseAndValidateAddresses(</span><br><span class="line">            config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),</span><br><span class="line">            config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取元数据</span></span><br><span class="line">        <span class="keyword">if</span> (metadata != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.metadata = metadata;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.metadata = <span class="keyword">new</span> ProducerMetadata(retryBackoffMs,</span><br><span class="line">                                                 config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),</span><br><span class="line">                                                 config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),</span><br><span class="line">                                                 logContext,</span><br><span class="line">                                                 clusterResourceListeners,</span><br><span class="line">                                                 Time.SYSTEM);</span><br><span class="line">            <span class="keyword">this</span>.metadata.bootstrap(addresses);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// sender线程初始化</span></span><br><span class="line">        <span class="keyword">this</span>.sender = newSender(logContext, kafkaClient, <span class="keyword">this</span>.metadata);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将sender线程放到后台启动</span></span><br><span class="line">        String ioThreadName = NETWORK_THREAD_PREFIX + <span class="string">&quot; | &quot;</span> + clientId;</span><br><span class="line">        <span class="keyword">this</span>.ioThread = <span class="keyword">new</span> KafkaThread(ioThreadName, <span class="keyword">this</span>.sender, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 启动sender线程，执行run方法</span></span><br><span class="line">        <span class="keyword">this</span>.ioThread.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-sender线程初始化流程-amp-源码"><a href="#2-sender线程初始化流程-amp-源码" class="headerlink" title="2 sender线程初始化流程 &amp; 源码"></a>2 sender线程初始化流程 &amp; 源码</h5><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009121142.png"/>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Sender <span class="title">newSender</span><span class="params">(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 设置缓存请求的最大个数</span></span><br><span class="line">    <span class="keyword">int</span> maxInflightRequests = configureInflightRequests(producerConfig);</span><br><span class="line">    <span class="comment">// 发送数据到broker的请求超时时间，默认30s，30s内不应答则重试</span></span><br><span class="line">    <span class="keyword">int</span> requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="comment">// 创建客户端对象，处理发送请求（客户端id，缓存最大请求个数，重试时间，发送缓冲区大小，接收缓冲区大小</span></span><br><span class="line">    KafkaClient client = kafkaClient != <span class="keyword">null</span> ? kafkaClient : <span class="keyword">new</span> NetworkClient(</span><br><span class="line">        <span class="keyword">new</span> Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),</span><br><span class="line">                     <span class="keyword">this</span>.metrics, time, <span class="string">&quot;producer&quot;</span>, channelBuilder, logContext),</span><br><span class="line">        metadata,</span><br><span class="line">        clientId,</span><br><span class="line">        maxInflightRequests,</span><br><span class="line">        producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">        producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),</span><br><span class="line">        producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">        producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">        requestTimeoutMs,</span><br><span class="line">        producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),</span><br><span class="line">        producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),</span><br><span class="line">        time,</span><br><span class="line">        <span class="keyword">true</span>,</span><br><span class="line">        apiVersions,</span><br><span class="line">        throttleTimeSensor,</span><br><span class="line">        logContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 应答级别设置</span></span><br><span class="line">    <span class="comment">// acks 默认值是 1 。</span></span><br><span class="line">    <span class="comment">// acks=0, 生产者发送给 Kafka 服务器后，不需要应答</span></span><br><span class="line">    <span class="comment">// acks=1 ，生产者发送给 Kafka 服务器后， Leader 接收后应答</span></span><br><span class="line">    <span class="comment">// acks=-1或all，生产者发送给 Kafka 服务器后， Leader 和在 ISR 队列的所有 Follower 共同应答</span></span><br><span class="line">    <span class="keyword">short</span> acks = configureAcks(producerConfig, log);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建sender线程</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Sender(logContext,</span><br><span class="line">                      client,</span><br><span class="line">                      metadata,</span><br><span class="line">                      <span class="keyword">this</span>.accumulator,</span><br><span class="line">                      maxInflightRequests == <span class="number">1</span>,</span><br><span class="line">                      producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),</span><br><span class="line">                      acks,</span><br><span class="line">                      producerConfig.getInt(ProducerConfig.RETRIES_CONFIG),</span><br><span class="line">                      metricsRegistry.senderMetrics,</span><br><span class="line">                      time,</span><br><span class="line">                      requestTimeoutMs,</span><br><span class="line">                      producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),</span><br><span class="line">                      <span class="keyword">this</span>.transactionManager,</span><br><span class="line">                      apiVersions);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-4-2-生产者发送数据到缓冲队列"><a href="#3-4-2-生产者发送数据到缓冲队列" class="headerlink" title="3.4.2 生产者发送数据到缓冲队列"></a>3.4.2 生产者发送数据到缓冲队列</h4><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009121513.png"/>

<p>核心流程：</p>
<ul>
<li>分区操作采用上述的分区分配方法；</li>
<li>校验发送的消息是否超过最大值，超过则直接抛异常；</li>
<li>像缓冲区（双端队列RecordAccumulator）追加数据；</li>
<li>发送条件满足，则唤醒sender线程进行发送；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Future&lt;RecordMetadata&gt; <span class="title">doSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">    TopicPartition tp = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 获取元数据，需要发送消息的主题、分区等信息</span></span><br><span class="line">            clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">            <span class="comment">// ......</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// key的序列化操作</span></span><br><span class="line">        <span class="keyword">byte</span>[] serializedKey;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert key of class &quot;</span> + record.key().getClass().getName() +</span><br><span class="line">                                             <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                             <span class="string">&quot; specified in key.serializer&quot;</span>, cce);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// value的序列化操作</span></span><br><span class="line">        <span class="keyword">byte</span>[] serializedValue;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert value of class &quot;</span> + record.value().getClass().getName() +</span><br><span class="line">                                             <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                             <span class="string">&quot; specified in value.serializer&quot;</span>, cce);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 分区操作</span></span><br><span class="line">        <span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">        tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// 保证数据大小能够传输（校验的是序列化和压缩之后的数据大小）</span></span><br><span class="line">        <span class="keyword">int</span> serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),</span><br><span class="line">                                                                           compressionType, serializedKey, serializedValue, headers);</span><br><span class="line">        ensureValidRecordSize(serializedSize);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// 像缓冲队列RecordAccumulator中追加数据</span></span><br><span class="line">        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                                                                         serializedValue, headers, interceptCallback, remainingWaitMs, <span class="keyword">true</span>, nowMs);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (result.abortForNewBatch) &#123;</span><br><span class="line">            <span class="keyword">int</span> prevPartition = partition;</span><br><span class="line">            partitioner.onNewBatch(record.topic(), cluster, prevPartition);</span><br><span class="line">            partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">            tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line">            <span class="keyword">if</span> (log.isTraceEnabled()) &#123;</span><br><span class="line">                log.trace(<span class="string">&quot;Retrying append due to new batch creation for topic &#123;&#125; partition &#123;&#125;. The old partition was &#123;&#125;&quot;</span>, record.topic(), partition, prevPartition);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 拦截器的回调函数</span></span><br><span class="line">            interceptCallback = <span class="keyword">new</span> KafkaProducer.InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line"></span><br><span class="line">            result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                                        serializedValue, headers, interceptCallback, remainingWaitMs, <span class="keyword">false</span>, nowMs);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">            transactionManager.maybeAddPartitionToTransaction(tp);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小已经满了，或有一个新的批次创建</span></span><br><span class="line">        <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">            log.trace(<span class="string">&quot;Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch&quot;</span>, record.topic(), partition);</span><br><span class="line">            <span class="comment">// 唤醒sender发送线程</span></span><br><span class="line">            <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result.future;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ApiException e) &#123;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>内存池 accumulator.append：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> RecordAccumulator.<span class="function">RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   Header[] headers,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   <span class="keyword">long</span> maxTimeToBlock,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   <span class="keyword">boolean</span> abortOnNewBatch,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                   <span class="keyword">long</span> nowMs)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 获取或给每个主题的分区创建队列</span></span><br><span class="line">        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">&quot;Producer closed while send in progress&quot;</span>);</span><br><span class="line">            <span class="comment">// 尝试向队列中添加数据</span></span><br><span class="line">            RecordAccumulator.RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// 申请内存（内存池分配内存</span></span><br><span class="line">        buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Update the current time in case the buffer allocation blocked above.</span></span><br><span class="line">        nowMs = time.milliseconds();</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="comment">// Need to check if producer is closed again after grabbing the dequeue lock.</span></span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">&quot;Producer closed while send in progress&quot;</span>);</span><br><span class="line"></span><br><span class="line">            RecordAccumulator.RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// Somebody else found us a batch, return the one we waited for! Hopefully this doesn&#x27;t happen often...</span></span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 封装内存buffer</span></span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">            <span class="comment">// 再次封装</span></span><br><span class="line">            ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, nowMs);</span><br><span class="line">            FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,</span><br><span class="line">                                                                                 callback, nowMs));</span><br><span class="line">            <span class="comment">// 向队列末尾添加批次</span></span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Don&#x27;t deallocate this buffer in the finally block as it&#x27;s being used in the record batch</span></span><br><span class="line">            buffer = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAccumulator.RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>, <span class="keyword">false</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="keyword">null</span>)</span><br><span class="line">            free.deallocate(buffer);</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-4-3-sender线程发送数据到kafka集群"><a href="#3-4-3-sender线程发送数据到kafka集群" class="headerlink" title="3.4.3 sender线程发送数据到kafka集群"></a>3.4.3 sender线程发送数据到kafka集群</h4><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009121558.png"/>

<p>Sender类继承了Runnable接口，唤醒sender线程时调用run()方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">runOnce</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 事务相关操作</span></span><br><span class="line">    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">long</span> currentTimeMs = time.milliseconds();</span><br><span class="line">    <span class="comment">// 发送数据</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line">    <span class="comment">// 获取发送结果</span></span><br><span class="line">    client.poll(pollTimeout, currentTimeMs);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>核心流程：</p>
<ul>
<li>sendProducerData() 发送数据</li>
<li>client.poll() 获取服务端的响应结果</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">sendProducerData</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取元数据（topic、分区信息等）</span></span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断32M缓存是否准备好，返回准备好的数据</span></span><br><span class="line">    RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发往同一个broker节点的数据，打包为一个请求批次，后续以节点为单位发送数据请求</span></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes, <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="comment">// 发送请求</span></span><br><span class="line">    sendProduceRequests(batches, now);</span><br><span class="line">    <span class="keyword">return</span> pollTimeout;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* sendProduceRequests() 发送请求 */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;ProducerBatch&gt; batches)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建客户端对象</span></span><br><span class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>, requestTimeoutMs, callback);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向服务端发送请求（通过selector把数据发送到集群）</span></span><br><span class="line">    client.send(clientRequest, now);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* client.send() 向服务端发送请求，底层调用doSend()方法 */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSend</span><span class="params">(ClientRequest clientRequest, <span class="keyword">boolean</span> isInternalRequest, <span class="keyword">long</span> now, AbstractRequest request)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    Send send = request.toSend(header);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加请求到InFlightRequests队列中</span></span><br><span class="line">    InFlightRequest inFlightRequest = <span class="keyword">new</span> InFlightRequest(</span><br><span class="line">        clientRequest,</span><br><span class="line">        header,</span><br><span class="line">        isInternalRequest,</span><br><span class="line">        request,</span><br><span class="line">        send,</span><br><span class="line">        now);</span><br><span class="line">    <span class="keyword">this</span>.inFlightRequests.add(inFlightRequest);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过selector向集群发送数据</span></span><br><span class="line">    selector.send(<span class="keyword">new</span> NetworkSend(clientRequest.destination(), send));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-5-生产者如何提高吞吐量"><a href="#3-5-生产者如何提高吞吐量" class="headerlink" title="3.5 生产者如何提高吞吐量"></a>3.5 生产者如何提高吞吐量</h3><ul>
<li>批次大小：默认为16k</li>
<li>缓冲时间：默认为0</li>
<li>压缩方式：默认为none</li>
<li>缓冲区大小：默认为32M</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 批次大小（默认为16k）/缓冲时间（默认为0ms）/压缩方式（默认为none,可设置gzip/snappy/lz4/zstd）/缓冲区大小，默认为32M</span></span><br><span class="line">properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line">properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, <span class="string">&quot;snappy&quot;</span>);</span><br><span class="line">properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br></pre></td></tr></table></figure>

<h2 id="四-Kafka消费者"><a href="#四-Kafka消费者" class="headerlink" title="四 Kafka消费者"></a>四 Kafka消费者</h2><h3 id="4-1-消费者-amp-消费者组"><a href="#4-1-消费者-amp-消费者组" class="headerlink" title="4.1 消费者 &amp; 消费者组"></a>4.1 消费者 &amp; 消费者组</h3><p>消费者负责订阅kafka中的主题，从订阅的主题上拉取消息，每一个消费者都有一个对应的消费者组，当消息发布到主题后，只会被投递给订阅它的每个消费者组中的一个消费者。</p>
<h4 id="4-1-1-每一个分区只能被消费者组中的一个消费者消费"><a href="#4-1-1-每一个分区只能被消费者组中的一个消费者消费" class="headerlink" title="4.1.1 每一个分区只能被消费者组中的一个消费者消费"></a>4.1.1 每一个分区只能被消费者组中的一个消费者消费</h4><ul>
<li>在一个消费者组中，一个消费者可以消费多个分区。</li>
<li>不同的消费者消费的分区一定不会重复，所有消费者一起消费所有的分区。</li>
<li>在不同消费者组中，每个消费者组都会消费所有的分区。</li>
<li>同一个消费者组下消费者对分区是互斥的，而不同消费组之间是共享的。</li>
</ul>
<p>例如：某个主题中共有4个分区(Partion): P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者(C0、C1、C2 、C3)，消费组B中有2个消费者(C4、C5)。<br>按照Kaflca默认的分区分配策略， 最后的分配结果是消费组A中的每一个消费者分配到1个分区， 消费组B中的每一个消费者分配到2个分区， 两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009141852.png" style="zoom:60%;" />

<h4 id="4-1-2-消费者组内的消费者个数变化时对应的分区分配的策略"><a href="#4-1-2-消费者组内的消费者个数变化时对应的分区分配的策略" class="headerlink" title="4.1.2 消费者组内的消费者个数变化时对应的分区分配的策略"></a>4.1.2 消费者组内的消费者个数变化时对应的分区分配的策略</h4><p>例如：假设目前某消费组内只有一个消费者C0，订阅了一个主题，这个主题包含7个分区：P0、P1、P2、P3、P4、P5、P6。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142024.png" style="zoom: 67%;" />

<p>此时消费者组内又加入了一个新的消费者C1、C2，此时按照默认得到分区分配策略，需要将原来的消费者 C0 的部分分区分配给消费者 C1、C2 消费。C0、C1、C2 各自消费所分配到的分区的消息，彼此之间并无逻辑上的干扰。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142130.png" style="zoom: 67%;" /><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142137.png" style="zoom: 67%;" /></p>
<p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。<br>但是对于分区数固定的清况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的清况，就会有消费者分配不到任何分区。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142222.png" style="zoom:67%;" />

<h4 id="4-1-3-分区分配策略"><a href="#4-1-3-分区分配策略" class="headerlink" title="4.1.3 分区分配策略"></a>4.1.3 分区分配策略</h4><p>kafka提供了消费者客户端参数 partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。默认情况下采用RangeAssignor分配策略。<br>除此之外， Kafka还提供了另外两种分配策略： RoundRobinAssignor和Sticky Assignor。消费者客户端可以配置多个分配策略。</p>
<h5 id="1-RangeAssignor分配策略"><a href="#1-RangeAssignor分配策略" class="headerlink" title="1  RangeAssignor分配策略"></a>1  RangeAssignor分配策略</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RangeAssignor分配策略的原理是：按照消费者总数和分区总数进行整除运算来获得一个跨度， 然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。</span><br></pre></td></tr></table></figure>

<ul>
<li><code>对于每一个主题，对每个分区进行排序</code></li>
<li><code>然后订阅这个Topic的消费组的消费者再进行排序，之后尽量均衡的将分区分配给消费者；</code></li>
<li><code>如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。</code></li>
</ul>
<p>【示例】：</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142422.png" style="zoom:67%;" />

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这种分配方式明显的一个问题是随着消费者订阅的Topic的数量的增加，不均衡的问题会越来越严重，比如上图中4个分区3个消费者的场景，C0会多分配一个分区。如果此时再订阅一个分区数为4的Topic，那么C0又会比C1、C2多分配一个分区，这样C0总共就比C1、C2多分配两个分区了，而且随着Topic的增加，这个情况会越来越严重。</span><br></pre></td></tr></table></figure>

<h5 id="2-RoundRobinAssignor分配策略"><a href="#2-RoundRobinAssignor分配策略" class="headerlink" title="2 RoundRobinAssignor分配策略"></a>2 RoundRobinAssignor分配策略</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RoundRobinAssignor分配策略的原理是：将消费组内所有消费者及消费者订阅的所有主题的分区排序，然后通过轮询方式逐个将分区依次分配给每个消费者。如果同一个消费组内所有的消费者的订阅信息都是相同的，那么 RoundRobinAssignor 分配策略的分区分配会是均匀的。</span><br></pre></td></tr></table></figure>

<ul>
<li><code>将消费组内订阅的所有Topic的分区及所有消费者进行排序（RangeAssignor是针对单个Topic的分区进行排序分配的）；</code></li>
<li><code>如果消费组内，消费者订阅的Topic列表是相同的（每个消费者都订阅了相同的Topic），那么分配结果是尽量均衡的（消费者之间分配到的分区数的差值不会超过1）；</code></li>
<li><code>如果订阅的Topic列表是不同的，那么分配结果是不保证“尽量均衡”的，因为某些消费者不参与一些Topic的分配；</code></li>
</ul>
<p>【示例】：</p>
<ul>
<li>对于订阅组内消费者订阅Topic一致的情况：</li>
</ul>
<p>假设有三个消费者分别为C0、C1、C2，订阅3个主题 T0、T1、T2，这3个主题分别拥有1、2、3个分区：</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142535.png" style="zoom:67%;" />

<ul>
<li>对于订阅组内消费者订阅Topic不一致的情况：</li>
</ul>
<p>假设有三个消费者分别为C0、C1、C2，有3个Topic T0、T1、T2，分别拥有1、2、3个分区，并且C0订阅T0，C1订阅T0和T1，C2订阅T0、T1、T0，那么RoundRobinAssignor的分配结果如下：</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142605.png" style="zoom:67%;" />

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果同一个消费组内的消费者订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能导致分区分配得不均匀。如果某个消费者没有订阅消费组内的某个主题，那么在分配分区的时候此消费者将分配不到这个主题的任何分区。&#96;&#96;可以看到 RoundRobinAssignor 策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区 t1p1 分配给消费者 C1。</span><br></pre></td></tr></table></figure>

<h4 id="3-StickyAssignor分配策略"><a href="#3-StickyAssignor分配策略" class="headerlink" title="3 StickyAssignor分配策略"></a>3 StickyAssignor分配策略</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sticky是“粘性的”，可以理解为分配结果是带“粘性的”——每一次分配变更相对上一次分配做最少的变动（上一次的结果是有粘性的），它主要有两个目的：</span><br><span class="line">1. 分区的分配要尽可能均匀。</span><br><span class="line">2. 分区的分配尽可能与上次分配的保持相同。</span><br><span class="line">当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor 分配策略的具体实现要比 RangeAssignor 和 RoundRobinAssignor 这两种分配策略要复杂得多。</span><br></pre></td></tr></table></figure>

<p>【示例】：</p>
<ul>
<li>对于订阅组内消费者订阅Topic一致的情况：</li>
</ul>
<p>假设消费组内有3个消费者（C0、C1 和 C2），它们都订阅了4个主题（t0、t1、t2、t3），并且每个主题有2个分区，所有消费者都订阅了 t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1 这8个分区。最终的分配结果如下：</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142717.png" style="zoom:67%;" />

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Sticky分配策略的初始分配结果和采用 RoundRobinAssignor 分配策略所分配的结果相同，但此时，假设此时消费者 C1 脱离了消费组，那么消费组就会执行再均衡操作，进而消费分区会重新分配。</span><br><span class="line">- 如果采用的是RoundRobinAssignor 分配策略，那么会按照消费者 C0 和 C2 进行重新轮询分配</span><br><span class="line">- 如果采用的是Sticky分配策略，可以看到分配结果中保留了上一次分配中对消费者 C0 和 C2 的所有分配结果，并将原来消费者 C1 的“负担”分配给了剩余的两个消费者 C0 和 C2，最终 C0 和 C2 的分配还保持了均衡。</span><br></pre></td></tr></table></figure>

<ul>
<li>对于订阅组内消费者订阅Topic不一致的情况：</li>
</ul>
<p>消费组内有3个消费者（C0、C1 和 C2），订阅3个主题（t0、t1 和 t2），这3个主题分别有1、2、3个分区。消费者 C0 订阅了主题 t0，消费者 C1 订阅了主题 t0 和 t1，消费者 C2 订阅了主题 t0、t1 和 t2。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009142912.png"/>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">分配结果如上图所示，StickyAssignor是比RangeAssignor和RoundRobinAssignor分配得更均衡。</span><br><span class="line">kafka默认采用Range + CooperativeSticky（和Sticky类似，只是支持了cooperative协议）相结合的分区策略。</span><br></pre></td></tr></table></figure>

<h3 id="4-2-消费者客户端使用"><a href="#4-2-消费者客户端使用" class="headerlink" title="4.2 消费者客户端使用"></a>4.2 消费者客户端使用</h3><h4 id="1-配置参数以及创建实例"><a href="#1-配置参数以及创建实例" class="headerlink" title="1 配置参数以及创建实例"></a>1 配置参数以及创建实例</h4><table>
<thead>
<tr>
<th>参数配置</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>指定连接kafka集群所需的broker地址列表，格式为host1:port1,host2:port2，可以设置一个或多个地址</td>
</tr>
<tr>
<td>group.id</td>
<td>消费者隶属的消费组的名称</td>
</tr>
<tr>
<td>key.deserializer</td>
<td>指定消息中key所需的反序列化操作的反序列化器（消费者从broker端获取的消息个事都是字节数组类型byte[]，需要执行相应的反序列化操作才能还原成原有的对象格式）</td>
</tr>
<tr>
<td>value.deserializer</td>
<td>指定消息中value所需的反序列化操作的反序列化器（Kaflca所提供的反序列化器有ByteBufferDeserializer、ByteArrayDeserializer、BytesDeserializer、DoubleDeserializer、FloatDeserializer、IntegerDeserializer、LongDeserializer、ShortDeserializer、StringDeserializer, 它们分别用千ByteBuffer、ByteArray、Bytes、Double、Float、Integer、Long、Short 及String 类型的反序列化）</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置必要的配置信息</span></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 创建 Consumer 实例</span></span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br></pre></td></tr></table></figure>

<h4 id="2-订阅主题"><a href="#2-订阅主题" class="headerlink" title="2 订阅主题"></a>2 订阅主题</h4><p>一个消费者可以订阅一个或多个主题，如果前后两次订阅了不同的主题，那么消费者以最后一次的为准。</p>
<table>
<thead>
<tr>
<th>订阅主题的方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>KafkaConsumer.subscribe(topics, new NoOpConsumerRebalanceListener())</td>
<td>订阅某些主题。subscribe()方法订阅主题具有消费者自动再均衡的功能，在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组内的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。</td>
</tr>
<tr>
<td>KafkaConsumer.assign()</td>
<td>订阅某些主题的特定分区。通过assign()方法订阅分区时，是不具备消费者自动均衡的功能的，这一点从assign()方法的参数中就可以看出，subscribe()方法有ConsumerRebalanceListener类型参数的方法，而assign()方法却没有。</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 订阅 Topic</span></span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>));</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 订阅某个主题的某个分区</span></span><br><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> TopicPartition(<span class="string">&quot;topic-demo&quot;</span>, <span class="number">0</span>)));</span><br></pre></td></tr></table></figure>

<h4 id="3-拉取消息并消费"><a href="#3-拉取消息并消费" class="headerlink" title="3 拉取消息并消费"></a>3 拉取消息并消费</h4><ul>
<li>Kafka中的消费是基于拉模式的。消息的消费一般有两种模式：推模式和拉模式。推模式是服务端主动将消息推送给消费者， 而拉模式是消费者主动向服务端发起请求来拉取消息。</li>
<li>Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用poll()方法，而poll()方法返回的是所订阅的主题（分区）上的一组消息。</li>
<li>poll()中的参数代表消费者缓冲区无数据时的阻塞时间，在消费者的缓冲区里没有可用数据时会发生阻塞。</li>
<li>poll()方法的返回值类型是ConsumerRecords，用来表示一次拉取操作所获得的消息集。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(<span class="number">3</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record: consumerRecords) &#123;</span><br><span class="line">        System.out.printf(<span class="string">&quot;offset: %d, key: %s, value: %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-提交消费位移"><a href="#4-提交消费位移" class="headerlink" title="4 提交消费位移"></a>4 提交消费位移</h4><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>KafkaConsumer.commitAsync()</td>
<td>异步提交；如果有coordinator，则创建提交消费位移的请求，通过回调函数，接收提交结果</td>
</tr>
<tr>
<td>consumer.commitSync()</td>
<td>同步提交；只要没超时，则在循环中一直发送提交请求直到提交成功</td>
</tr>
</tbody></table>
<ul>
<li>对于kafka消息而言，每条消息都有唯一的offset，用来表示消息在分区中对应的位置。消费者使用offset来表示消费到分区中某个消息所在的位置。</li>
<li>在每次调用poll()方法的时候，返回的是还没有被消费过的消息集，因此就需要记录上一次消费时的消费位移，并且这个消费位移必须做持久化保存，而不是单单保存在内存中，否则消费者重启之后就无法知晓之前的消费位移。</li>
<li>再考虑一种情况，当有新的消费者加入时，那么必然会有再均衡的动作， 对于同一分区而言，它可能在再均衡动作之后分配给新的消费者， 如果不持久化保存消费位移，那么这个新的消费者也无法知晓之前的消费位移。</li>
<li>在新消费者客户端中，消费位移存储在Kafka 内部的主题__consumer_offsets 中。这里把将消费位移存储起来（持久化）的动作称为“提交” ，消费者在消费完消息之后需要执行消费位移的提交。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(<span class="number">3</span>));</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="comment">// 异步提交offset</span></span><br><span class="line">    consumer.commitAsync();</span><br><span class="line">    <span class="comment">// 同步提交offset</span></span><br><span class="line">    consumer.commitSync();</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-消费者启动并拉取数据流程-amp-源码"><a href="#4-3-消费者启动并拉取数据流程-amp-源码" class="headerlink" title="4.3 消费者启动并拉取数据流程 &amp; 源码"></a>4.3 消费者启动并拉取数据流程 &amp; 源码</h3><h4 id="4-3-1-整体流程图"><a href="#4-3-1-整体流程图" class="headerlink" title="4.3.1 整体流程图"></a>4.3.1 整体流程图</h4><h5 id="1-消费者组初始化流程"><a href="#1-消费者组初始化流程" class="headerlink" title="1 消费者组初始化流程"></a>1 消费者组初始化流程</h5><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009143550.png"/>

<h5 id="2-消费者组详细消费的流程"><a href="#2-消费者组详细消费的流程" class="headerlink" title="2 消费者组详细消费的流程"></a>2 消费者组详细消费的流程</h5><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009143649.png"/>

<p>一个消费者组通常包含多个消费者实例，如果要达成上图所示的消费关系，必然需要一套机制来保证每个消费者知道自己应该从哪个分区拉消息，而这套机制的运作流程如下：<br>KafkaConsumer 中包含两个关键的组件：</p>
<ul>
<li>一个是负责协调消费者组内消费者分区消费的 ConsumerCoordinator：<ul>
<li><code>ConsumerCoordinator 会与 Kafka 的服务端交互，首先确定一个负责当前消费者组的消费者协调器，然后与这个协调器交互，加入消费者组并完成整个消费者组的消费分区的分配；</code></li>
</ul>
</li>
<li>另一个是负责拉取消息的 Fetcher；<ul>
<li><code>Fetcher 会在消费分区分配完成后向当前消费者负责的分区所在的 Broker 节点发起拉消息的请求；</code></li>
</ul>
</li>
</ul>
<p>整个消费者启动并拉取消息的核心流程可分为以下几步：</p>
<ol>
<li><code>消费者确定自己所在的消费者组协调器的地址，并与其建立连接。这个过程中，如果消费者需要更新 Kafka 集群元数据，则也进行处理；</code></li>
<li><code>消费者与协调器所在的服务端交互，请求加入消费者组。在这个过程中，协调器指定的 leader 将结合集群元数据与整个消费者组的消费者信息进行分区分配，完成后将分配方案发送给协调器；</code></li>
<li><code>协调器将分区分配方案返回给各个消费者，消费者向自己负责的分区所在的 Broker 发起拉消息请求，完成消息消费；</code></li>
<li><code>消费者会启动一个心跳线程与协调器保持连接，如果协调器返回消费者组状态变化，则进行重新加入消费者组的再均衡动作；</code></li>
</ol>
<h4 id="4-3-2-KafkaConsumer-的初始化"><a href="#4-3-2-KafkaConsumer-的初始化" class="headerlink" title="4.3.2 KafkaConsumer 的初始化"></a>4.3.2 KafkaConsumer 的初始化</h4><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/kafka客户端初始化.png" style="zoom:67%;" />

<p>KafkaConsumer()构造方法源码： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* KafkaConsumer.java */</span></span><br><span class="line">KafkaConsumer(ConsumerConfig config, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 获取元数据</span></span><br><span class="line">        <span class="comment">// 配置是否可以消费系统主题数据，默认为false</span></span><br><span class="line">        <span class="comment">// 配置是否允许自动创建主题，默认为true</span></span><br><span class="line">        <span class="keyword">this</span>.metadata = <span class="keyword">new</span> ConsumerMetadata(retryBackoffMs,</span><br><span class="line">                                             config.getLong(ConsumerConfig.METADATA_MAX_AGE_CONFIG),</span><br><span class="line">                                             !config.getBoolean(ConsumerConfig.EXCLUDE_INTERNAL_TOPICS_CONFIG),</span><br><span class="line">                                             config.getBoolean(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG),</span><br><span class="line">                                             subscriptions, logContext, clusterResourceListeners);</span><br><span class="line">        List&lt;InetSocketAddress&gt; addresses = ClientUtils.parseAndValidateAddresses(</span><br><span class="line">            config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG), config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG));</span><br><span class="line">        <span class="keyword">this</span>.metadata.bootstrap(addresses);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// 创建一个消费者而客户端，内部封装了一个 NetworkClient 对象，这个 NetworkClient 对象实际负责底层网络数据的读写</span></span><br><span class="line">        <span class="keyword">this</span>.client = <span class="keyword">new</span> ConsumerNetworkClient(</span><br><span class="line">            logContext,</span><br><span class="line">            netClient,</span><br><span class="line">            metadata,</span><br><span class="line">            time,</span><br><span class="line">            retryBackoffMs,</span><br><span class="line">            config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),</span><br><span class="line">            heartbeatIntervalMs); <span class="comment">//Will avoid blocking an extended period of time to prevent heartbeat thread starvation</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费者分区分配策略列表，当前消费者如被选为消费者组的 leader，将使用分配器进行分区分配</span></span><br><span class="line">        <span class="keyword">this</span>.assignors = ConsumerPartitionAssignor.getAssignorInstances(</span><br><span class="line">            config.getList(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG),</span><br><span class="line">            config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId))</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化消费者协调器，负责和服务端消费者组协调器交互</span></span><br><span class="line">        <span class="keyword">this</span>.coordinator = !groupId.isPresent() ? <span class="keyword">null</span> :</span><br><span class="line">        <span class="keyword">new</span> ConsumerCoordinator(groupRebalanceConfig,</span><br><span class="line">                                logContext,</span><br><span class="line">                                <span class="keyword">this</span>.client,</span><br><span class="line">                                assignors,</span><br><span class="line">                                <span class="keyword">this</span>.metadata,</span><br><span class="line">                                <span class="keyword">this</span>.subscriptions,</span><br><span class="line">                                metrics,</span><br><span class="line">                                metricGrpPrefix,</span><br><span class="line">                                <span class="keyword">this</span>.time,</span><br><span class="line">                                enableAutoCommit,</span><br><span class="line">                                config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG),</span><br><span class="line">                                <span class="keyword">this</span>.interceptors,</span><br><span class="line">                                config.getBoolean(ConsumerConfig.THROW_ON_FETCH_STABLE_OFFSET_UNSUPPORTED));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 抓取数据的配置，实际拉取服务端消息的组件</span></span><br><span class="line">        <span class="comment">// 配置一次抓取最小值/最大值/最大等待时间、每个分区抓取的最大字节数、一次poll拉取数据返回消息的最大条数、key和value的反序列化方法等</span></span><br><span class="line">        <span class="keyword">this</span>.fetcher = <span class="keyword">new</span> Fetcher&lt;&gt;(</span><br><span class="line">            logContext,</span><br><span class="line">            <span class="keyword">this</span>.client,</span><br><span class="line">            config.getInt(ConsumerConfig.FETCH_MIN_BYTES_CONFIG),</span><br><span class="line">            config.getInt(ConsumerConfig.FETCH_MAX_BYTES_CONFIG),</span><br><span class="line">            config.getInt(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG),</span><br><span class="line">            config.getInt(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG),</span><br><span class="line">            config.getInt(ConsumerConfig.MAX_POLL_RECORDS_CONFIG),</span><br><span class="line">            config.getBoolean(ConsumerConfig.CHECK_CRCS_CONFIG),</span><br><span class="line">            config.getString(ConsumerConfig.CLIENT_RACK_CONFIG),</span><br><span class="line">            <span class="keyword">this</span>.keyDeserializer,</span><br><span class="line">            <span class="keyword">this</span>.valueDeserializer,</span><br><span class="line">            <span class="keyword">this</span>.metadata,</span><br><span class="line">            <span class="keyword">this</span>.subscriptions,</span><br><span class="line">            metrics,</span><br><span class="line">            metricsRegistry,</span><br><span class="line">            <span class="keyword">this</span>.time,</span><br><span class="line">            <span class="keyword">this</span>.retryBackoffMs,</span><br><span class="line">            <span class="keyword">this</span>.requestTimeoutMs,</span><br><span class="line">            isolationLevel,</span><br><span class="line">            apiVersions);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-3-3-订阅主题"><a href="#4-3-3-订阅主题" class="headerlink" title="4.3.3 订阅主题"></a>4.3.3 订阅主题</h4><p>拉取消息前，消费者首先要声明自己订阅的主题，则KafkaConsumer.subscribe() 将被调用。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/kafka订阅主题完整.png" style="zoom:67%;" />

<p>订阅的主流程主要更新了两个属性：</p>
<ol>
<li>subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)：<ul>
<li><code>订阅主题（更新订阅状态 subscriptions，主要维护了订阅的 topic 和 patition 的消费位置等状态信息）</code></li>
<li><code>注册了一个负载均衡监听器（如果某个消费者挂了可以通知到其他消费者）</code></li>
</ul>
</li>
<li>metadata.requestUpdateForNewTopics()：<ul>
<li><code>更新元数据中的 topic 信息（属性 metadata 中维护了 Kafka 集群元数据的一个子集，包括集群的 Broker 节点、Topic 和 Partition 在节点上分布、Coordinator 给 Consumer 分配的 Partition 信息）</code></li>
</ul>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* KafkaConsumer.subscribe() 方法 */</span>     </span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">             *  Consumer不是线程安全的，在此获取一个轻量级的锁，确保只是单线程调用</span></span><br><span class="line"><span class="comment">             *  具体的实现是通过acquire()和release()接口，通过currentThread原子量进行一个状态的判断</span></span><br><span class="line"><span class="comment">             *      currentThread初始化为NO_CURRENT_THREAD，在release接口再次设置为NO_CURRENT_THREAD</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             *  acquireAndEnsureOpen()接口如果是多线程调用则抛出异常</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">    <span class="comment">// 确保当前消费者只有一个线程在操作，根据当前持有者线程currentThread和引用数refcount实现了线程安全，在消费或订阅的方法中都会调用这个方法。</span></span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 检查是否设置了groupId，未设置则会抛出异常</span></span><br><span class="line">        maybeThrowInvalidGroupIdException();</span><br><span class="line">        <span class="keyword">if</span> (topics == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Topic collection to subscribe to cannot be null&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (topics.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">// treat subscribing to empty topic list as the same as unsubscribing</span></span><br><span class="line">            <span class="comment">// 列表为空则表示取消订阅</span></span><br><span class="line">            <span class="keyword">this</span>.unsubscribe();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (String topic : topics) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Utils.isBlank(topic))</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 检测是否分区分配方式，默认是RangeAssignor， assignor不可以为空</span></span><br><span class="line">            throwIfNoAssignorsConfigured();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 清空非订阅分区拉取到的数据，避免消费错误</span></span><br><span class="line">            fetcher.clearBufferedDataForUnassignedTopics(topics);</span><br><span class="line">            log.info(<span class="string">&quot;Subscribed to topic(s): &#123;&#125;&quot;</span>, Utils.join(topics, <span class="string">&quot;, &quot;</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">                     *</span></span><br><span class="line"><span class="comment">                     * 更新订阅状态subscriptions，该订阅状态主要是维护：</span></span><br><span class="line"><span class="comment">                     *  1、设置 RebalanceListener,默认为NoOpConsumerRebalanceListener</span></span><br><span class="line"><span class="comment">                     *  2、订阅类型subscriptionType为AUTO_TOPICS</span></span><br><span class="line"><span class="comment">                     *  3、订阅主题subscription</span></span><br><span class="line"><span class="comment">                     *  2、订阅的 topic和partition 的消费位置等状态信息</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.subscribe(<span class="keyword">new</span> HashSet&lt;&gt;(topics), listener))</span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                     * 更新元数据metadata，此次操作并未发送更新请求，而只是将needPartialUpdate设置为true，lastRefreshMs设置为0来寻求立刻更新元数据主要是维护:</span></span><br><span class="line"><span class="comment">                     * Kafka集群元数据的一个子集，包括集群Broker节点、Topic和Partition在节点上分布</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line">                metadata.requestUpdateForNewTopics();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// 释放轻量级的锁，也就是将currentThread设置为NO_CURRENT_THREAD</span></span><br><span class="line">        release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>更新元数据：</p>
<ul>
<li>调用 Metadata.requestUpdateForNewTopics() 方法设置更新元数据的标识位 needPartialUpdate 为 true，这里面并没有真正发送更新元数据的请求。</li>
<li>Kafka 必须确保在第一次拉消息之前元数据是可用的，也就是说在第一次拉消息之前必须更新一次元数据，否则 Consumer 就不知道它应该去哪个 Broker 上去拉哪个 Partition 的消息。</li>
<li>在订阅的实现过程中，Kafka 更新了订阅状态 subscriptions 和元数据 metadata 中的相关 topic 的一些属性，将元数据状态置为“需要立即更新”，但是并没有真正发送更新元数据的请求，整个过程没有和集群有任何网络数据交换。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">requestUpdateForNewTopics</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 覆盖上次更新的时间戳来立刻更新</span></span><br><span class="line">    <span class="keyword">this</span>.lastRefreshMs = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 设置更新元数据标志位</span></span><br><span class="line"><span class="comment">     *  Kafka在pull数据之前需要对元数据进行更新</span></span><br><span class="line"><span class="comment">     *  否则 Consumer 就不知道它应该去哪个Broker上拉哪个Partition的消息。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">this</span>.needPartialUpdate = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">this</span>.requestVersion++;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.updateVersion;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-3-4-拉取消息的实现"><a href="#4-3-4-拉取消息的实现" class="headerlink" title="4.3.4 拉取消息的实现"></a>4.3.4 拉取消息的实现</h4><p>设置订阅的主题后，则可调用 KafkaConsumer.poll() 方法进入拉消息的流程，此处是消息消费的入口，核心流程是先后调用了2个方法：</p>
<ul>
<li><code>调用 KafkaConsumer.updateAssignmentMetadataIfNeeded() 方法进入消费者分区分配的流程</code></li>
<li><code>调用 KafkaConsumer.pollForFetches() 方法拉取消息</code></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/kafka拉取数据.png"/>

<h5 id="1-updateAssignmentMetadataIfNeeded-消费者分区分配"><a href="#1-updateAssignmentMetadataIfNeeded-消费者分区分配" class="headerlink" title="1 updateAssignmentMetadataIfNeeded()消费者分区分配"></a>1 updateAssignmentMetadataIfNeeded()消费者分区分配</h5><ul>
<li><h6 id="ConsumerCoordinator-poll-消费者-消费者组初始化"><a href="#ConsumerCoordinator-poll-消费者-消费者组初始化" class="headerlink" title="ConsumerCoordinator.poll()消费者/消费者组初始化"></a>ConsumerCoordinator.poll()消费者/消费者组初始化</h6></li>
</ul>
<p>updateAssignmentMetadataIfNeeded()方法的核心是调用 ConsumerCoordinator.poll()方法与服务端协调器交互，确定当前消费者负责的分区，为拉取消息做准备。</p>
<p>源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* coordinator.poll()方法 */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">poll</span><span class="params">(Timer timer, <span class="keyword">boolean</span> waitForJoinGroup)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取最新的元数据</span></span><br><span class="line">    maybeUpdateSubscriptionMetadata();</span><br><span class="line">    <span class="comment">// 执行已完成(异步提交)的 offset 提交请求的回调函数。</span></span><br><span class="line">    invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果指定了自动分区分配</span></span><br><span class="line">    <span class="keyword">if</span> (subscriptions.hasAutoAssignedPartitions()) &#123;</span><br><span class="line">        <span class="comment">// 如果没有指定分区分配策略，直接抛出异常</span></span><br><span class="line">        <span class="keyword">if</span> (protocol == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;User configured &quot;</span> + ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG +</span><br><span class="line">                                            <span class="string">&quot; to empty while trying to subscribe for group protocol to auto assign partitions&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费者需要定期发送心跳请求（Heartbeat Request）到Broker端的协调者以证明他还存活，确保不会被“踢出”消费组。</span></span><br><span class="line">        <span class="comment">// 检查心跳线程运行是否正常，如果心跳线程失败则抛出异常，反之则更新 poll 调用时间。</span></span><br><span class="line">        pollHeartbeat(timer.currentTimeMs());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保证和coordinator正常通信（寻找服务端的coordinator）</span></span><br><span class="line">        <span class="comment">// 如果找到 coordinator，直接返回；如果没找到，循环给服务端发送寻找coordinator的请求，直到找到coordinator；</span></span><br><span class="line">        <span class="comment">// 如果不存在协调器和协调器已断开连接，则返回 false，结束本次拉取。如果协调器就绪，则继续往下走。</span></span><br><span class="line">        <span class="keyword">if</span> (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否需要触发重平衡，即消费组内的所有消费者重新分配 topic 中的分区信息</span></span><br><span class="line">        <span class="keyword">if</span> (rejoinNeededOrPending()) &#123;</span><br><span class="line">            <span class="comment">// 如果是按正则表达式匹配的topic，则拉取和更新元数据</span></span><br><span class="line">            <span class="keyword">if</span> (subscriptions.hasPatternSubscription()) &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.metadata.timeToAllowUpdate(timer.currentTimeMs()) == <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (!client.ensureFreshMetadata(timer)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                maybeUpdateSubscriptionMetadata();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!ensureActiveGroup(waitForJoinGroup ? timer : time.timer(<span class="number">0L</span>))) &#123;</span><br><span class="line">                timer.update(time.milliseconds());</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果开启了自动提交消费进度，并且已到下一次提交时间，则提交</span></span><br><span class="line">    maybeAutoCommitOffsetsAsync(timer.currentTimeMs());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>核心流程：</p>
<ol>
<li>调用 **ensureCoordinatorReady()**方法来确保和Coordinator正常通信：<ul>
<li><code>调用lookupCoordinator() 方法，选择一个连接最小的节点，创建 FindCoordinator 请求并发送，请求数据信息，并对 response 进行处理：</code></li>
<li><code>进一步调用sendFindCoordinatorRequest() 方法，会使用 group id 通过 ConsumerNetworkClient.send() 来查找对应的 GroupCoordinator 节点：</code></li>
<li><code>如果正确获取 GroupCoordinator 时（会返回其对应的 node id、host 和 port 信息），建立连接，并更新心跳时间；</code></li>
</ul>
</li>
<li>调用**rejoinNeededOrPending()**方法判断是否触发重平衡，下列几种情况会触发再均衡 reblance 操作：<ul>
<li><code>新的消费者加入消费组 (第一次进行消费也属于这种情况)</code></li>
<li><code>消费者宕机下线 (长时间未发送心跳包)</code></li>
<li><code>消费者主动退出消费组，比如调用 unsubscrible() 方法取消对主题的订阅</code></li>
<li><code>消费组对应的 GroupCoordinator 节点发生了变化</code></li>
<li><code>消费组内所订阅的任一主题或者主题的分区数量发生了变化</code></li>
</ul>
</li>
<li>如果是按正则表达式匹配订阅的topic，则调用**ConsumerNetworkClient.ensureFreshMetadata()**方法，来拉取和更新元数据：<ul>
<li><code>ConsumerNetworkClient.poll()方法，实现了与 Cluster 通信，在 Coordinator 上注册 Consumer 并拉取和更新元数据。元数据此时会真正做一次更新；</code></li>
<li><code>类 ConsumerNetworkClient 封装了 Consumer 和 Cluster 之间所有的网络通信的实现，这个类是一个非常彻底的异步实现。它没有维护任何的线程，所有待发送的 Request 都存放在属性 unsent 中，返回的 Response 存放在属性 pendingCompletion 中。每次调用 poll() 方法的时候，在当前线程中发送所有待发送的 Request，处理所有收到的 Response。</code></li>
</ul>
</li>
<li>调用**ensureActiveGroup()**方法来加入一个消费者组：<ul>
<li><code>调用ensureCoordinatorReady()方法，确保 GroupCoordinator 已经连接；</code></li>
<li><code>调用startHeartbeatThreadIfNeeded()方法，启动心跳发送线程（并不一定发送心跳,满足条件后才会发送心跳）；</code></li>
<li><code>调用joinGroupIfNeeded()方法，发送 JoinGroup 请求，并对返回的信息进行处理；</code></li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一个 consumer 实例算是真正上意义上加入 group 成功。然后消费者就进入正常工作状态，同时消费者也通过向 GroupCoordinator 发送心跳来维持它们与消费者的从属关系以及它们对分区的所有权关系。只要以正常的间隔发送心跳，就被认为是活跃的，但是如果 GroupCoordinator 没有响应，那么就会发送 LeaveGroup 请求退出消费组。</span><br></pre></td></tr></table></figure>

<h5 id="2-KafkaConsumer-pollForFetches-开始拉取数据"><a href="#2-KafkaConsumer-pollForFetches-开始拉取数据" class="headerlink" title="2 KafkaConsumer.pollForFetches() 开始拉取数据"></a>2 KafkaConsumer.pollForFetches() 开始拉取数据</h5><p>源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开始拉取数据</span></span><br><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123;</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="comment">// 如果缓存里面有未读取的消息，直接返回这些消息</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class="line">    <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> records;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 构造拉取消息请求，并发送请求，开始拉取数据</span></span><br><span class="line">    fetcher.sendFetches();</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="comment">// 发送网络请求拉取消息，会真正将之前构造的所有 Request 发送出去，等待直到有消息返回或者超时</span></span><br><span class="line">    client.poll(pollTimer, () -&gt; &#123;</span><br><span class="line">        <span class="keyword">return</span> !fetcher.hasCompletedFetches();</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    <span class="comment">// 从completedFetches 队列中取数，返回拉到的消息，把数据按照分区封装好，反序列化后返回，一次默认拉取500条数据</span></span><br><span class="line">    <span class="keyword">return</span> fetcher.fetchedRecords();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>核心流程：</p>
<ol>
<li>调用 <strong>Fetcher.sendFetches()</strong> 方法向服务端发起请求获取消息，流程是：<ul>
<li><code>初始化抓取数据的参数，构造到所有需要的 Broker 的拉消息的请求 FetchRequest；</code></li>
<li><code>调用ConsumerNetworkClient.send() 方法将这些请求异步发送出去；</code></li>
<li><code>注册一个回调类来处理返回的结果，所有返回的 Response 被暂时存放在 Fetcher.completedFetches 队列中；</code></li>
</ul>
</li>
<li>调用<strong>ConsumerNetworkClient.poll()</strong> 方法时，会真正将之前构造的所有 Request 发送出去，并处理收到的响应数据；</li>
<li>调用 **fetcher.fetchedRecords()**方法，将返回的数据反序列化后转换为消息列表，返回给调用者。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    sensors.maybeUpdateAssignment(subscriptions);</span><br><span class="line"></span><br><span class="line">    Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; fetchRequestMap = prepareFetchRequests();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData&gt; entry : fetchRequestMap.entrySet()) &#123;</span><br><span class="line">        <span class="keyword">final</span> Node fetchTarget = entry.getKey();</span><br><span class="line">        <span class="keyword">final</span> FetchSessionHandler.FetchRequestData data = entry.getValue();</span><br><span class="line">        <span class="comment">// 初始化抓取数据的参数：最大等待时间、最小抓取一个字节数据、最大抓取50M数据</span></span><br><span class="line">        <span class="keyword">final</span> FetchRequest.Builder request = FetchRequest.Builder</span><br><span class="line">            .forConsumer(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, data.toSend())</span><br><span class="line">            .isolationLevel(isolationLevel)</span><br><span class="line">            .setMaxBytes(<span class="keyword">this</span>.maxBytes)</span><br><span class="line">            .metadata(data.metadata())</span><br><span class="line">            .toForget(data.toForget())</span><br><span class="line">            .rackId(clientRackId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用client.send() 方法异步发送请求</span></span><br><span class="line">        RequestFuture&lt;ClientResponse&gt; future = client.send(fetchTarget, request);</span><br><span class="line">        <span class="keyword">this</span>.nodesWithPendingFetchRequests.add(entry.getKey().id());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 监听服务器返回的数据，注册一个回调函数处理返回的response</span></span><br><span class="line">        future.addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">synchronized</span> (Fetcher.<span class="keyword">this</span>) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="comment">// 成功获取服务端的响应数据</span></span><br><span class="line">                        FetchResponse response = (FetchResponse) resp.responseBody();</span><br><span class="line">                        FetchSessionHandler handler = sessionHandler(fetchTarget.id());</span><br><span class="line">                        <span class="comment">// ......</span></span><br><span class="line">                        Set&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> HashSet&lt;&gt;(response.responseData().keySet());</span><br><span class="line">                        Fetcher.FetchResponseMetricAggregator metricAggregator = <span class="keyword">new</span> Fetcher.FetchResponseMetricAggregator(sensors, partitions);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, FetchResponseData.PartitionData&gt; entry : response.responseData().entrySet()) &#123;</span><br><span class="line">                            TopicPartition partition = entry.getKey();</span><br><span class="line">                            FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);</span><br><span class="line">                            <span class="keyword">if</span> (requestData == <span class="keyword">null</span>) &#123;</span><br><span class="line">                                <span class="comment">// ......</span></span><br><span class="line">                                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(message);</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                <span class="comment">// ......</span></span><br><span class="line">                                <span class="comment">// 把数据按照分区，添加到消息队列中completedFetches</span></span><br><span class="line">                                completedFetches.add(<span class="keyword">new</span> Fetcher.CompletedFetch(partition, partitionData,</span><br><span class="line">                                                                                metricAggregator, batches, fetchOffset, responseVersion));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        sensors.fetchLatency.record(resp.requestLatencyMs());</span><br><span class="line">                    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                        nodesWithPendingFetchRequests.remove(fetchTarget.id());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// ......</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fetchRequestMap.size();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="五-Kafka分区和副本"><a href="#五-Kafka分区和副本" class="headerlink" title="五 Kafka分区和副本"></a>五 Kafka分区和副本</h2><p>从Kafka 的底层实现来说，主题和分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段（ LogSegment ），每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。</p>
<h3 id="5-1-分区副本的分配"><a href="#5-1-分区副本的分配" class="headerlink" title="5.1 分区副本的分配"></a>5.1 分区副本的分配</h3><p>生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区，而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker 中创建哪些分区的副本。</p>
<p>同一个分区中的多个副本必须分布在不同的broker中，这样才能提供有效的数据冗余。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009170040.png"/>

<p>在创建主题时，如果使用了replica-assignment 参数，那么就按照指定的方案来进行分区副本的创建；如果没有使用replica-assignment 参数，那么就需要按照内部的逻辑来计算分配方案了。</p>
<p>使用kafka-topics.sh 脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略： 未指定机架信息和指定机架信息。如果集群中所有的broker节点都没有配置broker.rack 参数，或者使用disable-rack-aware 参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。</p>
<p>目前kafka只支持增加分区数，不支持减少分区数，原因是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">按照Kafka现有的代码逻辑， 此功能完全可以实现，不过也会使代码的复杂度急剧增大。实现此功能需要考虑的因素很多， 比如：</span><br><span class="line">删除的分区中的消息该如何处理？如果随着分区一起消失则消息的可靠性得不到保障；如果需要保留则又需要考虑如何保留。直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于Spark、Flink这类需要消息时间戳（事件时间）的组件将会受到影响；如果分散插入现有的分区，那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障？与此同时， 顺序性问题、事务性问题，以及分区和副本的状态机切换问题都是不得不面对的。</span><br><span class="line">反观这个功能的收益点却是很低的，如果真的需要实现此类功能，则完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去即可。</span><br></pre></td></tr></table></figure>

<h3 id="5-2-kafka副本"><a href="#5-2-kafka副本" class="headerlink" title="5.2 kafka副本"></a>5.2 kafka副本</h3><p>分区使用多副本机制来提升可靠性， 但只有leader副本对外提供读写服务， 而follower副本只负责在内部进行消息的同步。如果一个分区的leader副本不可用，那么就意味着整个分区变得不可用，此时就需要Kafka从剩余的follower副本中挑选一个新的leader副本来继续对外提供服务。虽然不够严谨， 但从某种程度上说，broker节点中leader副本个数的多少决定了这个节点负载的高低。</p>
<p>当分区的leader节点发生故障时，其中一个follower节点会成为新的leader节点，这就会导致集群的负载不均衡，从而影响整体的健壮性和稳定性。当原来的leader节点恢复之后重新加入集群时，它只能成为一个新的follower节点而不再对外提供服务。</p>
<p>Kafka 副本作用：提高数据可靠性 。</p>
<ul>
<li><p>Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；太多副本会增加磁盘存储空间，增加网络上数据传输，降低效率。</p>
</li>
<li><p>Kafka 中副本分为： Leader 和 Follower 。 Kafka 生产者只会把数据发往 Leader，然后 Follower 找 Leader 进行同步数据。</p>
</li>
<li><p>Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">AR &#x3D; ISR + OSR</span><br><span class="line"></span><br><span class="line">- ISR </span><br><span class="line">表示和Leader保持同步的Follower集合。如果Follower长时间未向Leader发送通信请求或同步数据，则该Follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定，默认30s。Leader发生故障之后，就会从ISR中选举新的Leader。</span><br><span class="line"></span><br><span class="line">- OSR</span><br><span class="line">表示Follower与Leader副本同步时，延迟过多的副本。</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="1-Leader-的选举"><a href="#1-Leader-的选举" class="headerlink" title="1 Leader 的选举"></a>1 Leader 的选举</h5><p>Kafka 集群中有一个broker 的Controller 会被选举为Controller Leader，负责管理集群broker 的上下线，所有topic 的分区副本分配和Leader 选举等工作。Controller 的信息同步工作是依赖于Zookeeper 的。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009173027.png" style="zoom:80%;" />

<h5 id="2-Leader和Follower故障处理细节"><a href="#2-Leader和Follower故障处理细节" class="headerlink" title="2 Leader和Follower故障处理细节"></a>2 Leader和Follower故障处理细节</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。</span><br><span class="line">HW（High Watermark）：所有副本中最小的LEO</span><br></pre></td></tr></table></figure>

<ul>
<li>Follower故障</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009173204.png" style="zoom:67%;" />

<ul>
<li>Leader故障</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009173218.png" style="zoom:67%;" />

<h3 id="5-2-Kafka文件存储机制"><a href="#5-2-Kafka文件存储机制" class="headerlink" title="5.2 Kafka文件存储机制"></a>5.2 Kafka文件存储机制</h3><h5 id="1-log-和-index-文件"><a href="#1-log-和-index-文件" class="headerlink" title="1 log 和 index 文件"></a>1 log 和 index 文件</h5><p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment包括：<code>.index</code>文件、<code>.log</code>文件和<code>.timeindex</code>等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。</p>
<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20221009173803.png" style="zoom:67%;" />

<ul>
<li><p><code>topic</code>在物理层面以<code>partition</code>为分组，一个<code>topic</code>可以分成若干个<code>partition</code>，<code>partition</code>还可以细分为<code>Segment</code>，一个<code>partition</code>物理上由多个<code>Segment</code>组成。<code>segment</code>的参数有两个：</p>
<ul>
<li><code>log.segment.bytes</code>: 单个<code>segment</code>可容纳的最大数据量，默认为<code>1GB</code></li>
<li><code>log.segment.ms</code>: <code>Kafka</code>在<code>commit</code>一个未写满的<code>segment</code>前，所等待的时间(默认为7天)</li>
</ul>
</li>
<li><p><code>LogSegment</code>文件由两部分组成，分别为<code>.index</code>文件和<code>.log</code>文件，分别表示为<code>Segment</code>索引文件和数据文件。</p>
<ul>
<li><p><code>partition</code>全局的第一 个<code>segment</code>从0开始， 后续每个<code>segment</code>文件名为上一个<code>segment</code>文件最后一 条消息的<code>offset</code>值</p>
</li>
<li><p>数值大小为64位，20位数字字符长度，没有数字用0填充</p>
</li>
<li><p>```<br>第一个segment<br>00000000000000000000.index<br>00000000000000000000.1og<br>第二个segment，文件命名以第一个segment的最后一条消息的offset组成<br>00000000000000170410.index<br>00000000000000170410.1og<br>第三个segment，文件命名以上一个segment的最后一条消息的offset组成<br>00000000000000239430.index<br>00000000000000239430.1og</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 消息都具有固定的物理结构，包括: &#96;offset(8 Bytes)&#96;、消息体的大小&#96;(4 Bytes)&#96;、&#96;crc32(4 Bytes)&#96;、&#96;magic(1 Byte)&#96;、&#96;attributes(1 Byte)&#96;。&#96;key length(4 Bytes)&#96;、&#96;key(K Bytes)&#96;、&#96;payload(N Bytes)&#96;等等字段，可以确定一条消息的大小， 即读取到哪里截止。</span><br><span class="line"></span><br><span class="line">如何log和index文件定位到响应的记录？</span><br><span class="line"></span><br><span class="line">&lt;img src&#x3D;&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;varrella&#x2F;ImgHosting&#x2F;20221009173625.png&quot; style&#x3D;&quot;zoom:67%;&quot; &#x2F;&gt;</span><br><span class="line"></span><br><span class="line">##### 2 数据删除</span><br><span class="line"></span><br><span class="line">无论消息是否被消费，kafka都会保留所有消息，有两种策略可以删除旧数据：</span><br><span class="line"></span><br><span class="line">- 基于时间：log.retention.hours &#x3D; 168</span><br><span class="line">- 基于大小：log.retention.bytes &#x3D; 1073741824</span><br><span class="line"></span><br><span class="line">### 5.3 高效读写数据</span><br><span class="line"></span><br><span class="line">- Kafka 本身是分布式集群，可以采用分区技术，并行度高</span><br><span class="line">- 读数据采用稀疏索引，可以快速定位要消费的数据</span><br><span class="line">- 顺序写磁盘：Kafka 的producer 生产数据，要写入到log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M&#x2F;s，而随机写只有100K&#x2F;s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</span><br><span class="line"></span><br><span class="line">- 页缓存 + 零拷贝技术</span><br><span class="line"></span><br><span class="line">  - 页缓存：Kafka重度依赖底层操作系统提供的PageCache功能，当上层有写操作时，操作系统只是将数据写入PageCache；当读操作发生时，先从PageCache中查找，如果找不到，再去磁盘中读取。实际上PageCache是把尽可能多得到空闲内存都当做了磁盘缓存来使用。</span><br><span class="line">  - 零拷贝：Kafka的数据加工处理操作交由Kafka生产者和消费者的拦截器来处理。Kafka broker层不关心存储的数据，所以就不用走应用层，传输效率高，如图：</span><br><span class="line"></span><br><span class="line">  &lt;img src&#x3D;&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;varrella&#x2F;ImgHosting&#x2F;20221009174253.png&quot; style&#x3D;&quot;zoom:67%;&quot; &#x2F;&gt;</span><br><span class="line"></span><br><span class="line">## 六 数据的安全性</span><br><span class="line"></span><br><span class="line">### 6.1 数据可靠性-ACK应答级别</span><br><span class="line"></span><br><span class="line">&#96;Producers&#96;可以选择是否为数据的写入接收&#96;ack&#96;，有以下集中&#96;ack&#96;的选项：&#96;request.required.acks&#96;</span><br><span class="line"></span><br><span class="line">- &#96;acks &#x3D; 0&#96;：&#96;Producer&#96;无需等待来自&#96;Broker&#96;的确认而继续发送下一批消息。可靠性差，效率高。数据可能丢失。</span><br><span class="line">- &#96;acks &#x3D; 1&#96;：&#96;Producer&#96;在&#96;ISR&#96;中的&#96;Leader&#96;已经成功收到数据并得到确认后，发送下一条消息。可靠性中等，效率中等。</span><br><span class="line">- &#96;acks &#x3D; -1(all)&#96;：&#96;Producer&#96;需要等待&#96;ISR&#96;中的所有&#96;Follower&#96;都确认接收到数据之后才算一次发送完成，可靠性最高，效率低。Leader维护了一个动态的in-sync replica set（ISR），意为和Leader保持同步的Follower+Leader集合(leader：0，isr:0,1,2)。如果Follower长时间未向Leader发送通信请求或同步数据，则该Follower将被踢出ISR。该时间阈值由replica.lag.time.max.ms参数设定，默认30s。例如2超时，(leader:0, isr:0,1)。这样就不用等长期联系不上或者已经故障的节点。如果分区副本设置为1个，或者ISR里应答的最小副本数量（ min.insync.replicas 默认为1）设置为1，和ack&#x3D;1的效果是一样的，仍然有丢数的风险（leader：0，isr:0）。</span><br><span class="line"></span><br><span class="line">在生产环境中，acks&#x3D;0很少使用；acks&#x3D;1，一般用于传输普通日志，允许丢个别数据；acks&#x3D;-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>数据完全可靠条件 = ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 6.2 数据重复性</span><br><span class="line"></span><br><span class="line">##### 1 数据传递语义</span><br><span class="line"></span><br><span class="line">如果将消费者位移设置为自动提交，那么消费者一旦读到数据立即自动提交，如果只讨论这一读取消息的过程，那kafka确保了&#96;Exactly once&#96;。</span><br><span class="line"></span><br><span class="line">- 读完消息先提交消费位移再处理消息：At most once</span><br><span class="line">  - 消费者先提交消费位移，但还未来得及处理消息，此时宕机，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，消息丢失，但不可能重复处理。</span><br><span class="line">- 读完消息先处理再提交：At least once</span><br><span class="line">  - 消费者先处理消息，在提交消费位移，此时宕机，下次重新开始工作时还会处理刚刚未提交的消息，但实际上该消息已经被处理过了。消息不会丢失，但重复处理。</span><br><span class="line">- 如果一定要做到&#96;Exactly once&#96;，就需要协调&#96;offset&#96;和实际操作的输出。</span><br><span class="line">  - 经典的做法是引入两阶段提交</span><br><span class="line">  - Exactly once：每条消息肯定会被传输一次，且仅传输一次，保证数据既不重复也不丢失</span><br><span class="line">- &#96;kafka&#96;默认保证&#96;At least once&#96;，并且允许通过设置&#96;producer&#96;异步提交来实现&#96;At most once&#96;。</span><br><span class="line"></span><br><span class="line">##### 2 数据的消费</span><br><span class="line"></span><br><span class="line">- &#96;pariton_num&#x3D;2&#96;，启动一个&#96;consumer&#96;进程订阅这个&#96;topic&#96;，对应的，&#96;stream_num&#96;设为2，也就是说启两个线程并行处理&#96;message&#96;。</span><br><span class="line">- 如果&#96;auto.commit.enable &#x3D; true&#96;</span><br><span class="line">  - 当&#96;consumer&#96; 拉取了一些数据但还没有完全处理掉的时候。</span><br><span class="line">  - 刚好到&#96;commit interval&#96;出发了提交&#96;offset&#96;操作，接着&#96;consumer&#96; crash掉了。</span><br><span class="line">  - 这时已经fetch的数据还没有处理完成但已经被&#96;commit&#96;掉，因此没有机会再次被处理，数据丢失。</span><br><span class="line">- 如果&#96;auto.commit.enable &#x3D; false&#96;</span><br><span class="line">  - 假设&#96;consumer&#96;的两个&#96;fetcher&#96;各自拿了一条数据， 并且由两个线程同时处理。</span><br><span class="line">  - 这时线程&#96;t1&#96;处理完&#96;partition1&#96;的数据，手动提交&#96;offset&#96;，这里需要着重说明的是，当手动执行&#96;commit&#96;的时候，实际上是对这个&#96;consumer&#96;进程所占有的所有&#96;partition&#96;进行&#96;commit&#96;，&#96;kafka&#96;暂时还没有提供更细粒度的&#96;commit&#96;方式。也就是说，即使&#96;t2&#96;没有处理完&#96;partition2&#96;的数据，&#96;offset&#96;也被&#96;t1&#96;提交掉了。 </span><br><span class="line">  - 如果这时&#96;consumer crash&#96;掉，&#96;t2&#96;正在处理的这条数据就丢失了。</span><br><span class="line">- 方法1: (将多线程问题转成单线程)</span><br><span class="line">  - 手动&#96;commit&#96; &#96;offset&#96;，并针对&#96;partition_num&#96;启同样数目的&#96;consumer&#96;进程，这样就能保证一个&#96;consumer&#96;进程占有一个&#96;partition&#96;。&#96;commit offset&#96;的时候不会影响别的&#96;parition&#96;的&#96;offset&#96;。</span><br><span class="line">  - 但这个方法比较局限，因为&#96;partition&#96;和&#96;consumer&#96;进程的数目必须严格对应。</span><br><span class="line">- 方法2: (参考HDFS数据写入流程)</span><br><span class="line">  - 手动&#96;commit offset&#96;，另外在&#96;consumer&#96;端再将所有&#96;fetch&#96;到的数据缓存到&#96;queue&#96;里，当把&#96;queue&#96;里所有的数据处理完之后，再批量提交&#96;offset&#96;，这样就能保证只有处理完的数据才被&#96;commit&#96;。</span><br><span class="line"></span><br><span class="line">##### 2 幂等性</span><br><span class="line"></span><br><span class="line">幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;bash</span><br><span class="line">精确一次（Exactly Once） &#x3D; 幂等性 + 至少一次（ack&#x3D;-1 + 分区副本数&gt;&#x3D;2 + ISR最小副本数量&gt;&#x3D;2）</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>重复数据的判断标准：具有<code>&lt;PID, Partition, SeqNumber&gt;</code>相同主键的消息提交时，Broker只会持久化一条。其中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。</p>
<p>所以幂等性只能保证的是在单分区单会话内不重复。使用幂等性：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">开启参数enable.idempotence 默认为<span class="literal">true</span>，<span class="literal">false</span> 关闭。</span><br></pre></td></tr></table></figure>

<h5 id="3-生产者事务"><a href="#3-生产者事务" class="headerlink" title="3 生产者事务"></a>3 生产者事务</h5><p>开启事务，必须先开启幂等性。</p>
<p>Producer 在使用事务功能前，必须先自定义一个唯一的<code>transactional.id</code>。有了<code>transactional.id</code>，即使客户端挂掉了，它重启后也能继续处理未完成的事务。</p>
<p>kafka的事务一共有5个API：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 初始化事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initTransactions</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 2 开启事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 3 在事务内提交已经消费的偏移量（主要用于消费者）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, String consumerGroupId)</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 4 提交事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"><span class="comment">// 5 放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br></pre></td></tr></table></figure>

<p>单个Producer，使用事务保证消息的仅一次发送。</p>
<h5 id="4-数据有序-amp-乱序"><a href="#4-数据有序-amp-乱序" class="headerlink" title="4 数据有序 &amp; 乱序"></a>4 数据有序 &amp; 乱序</h5><p>单分区内，有序（有条件的，详见下节）；<br>多分区，分区与分区间无序；</p>
<p>（1）kafka在1.x版本之前保证数据单分区有序，条件为：max.in.flight.requests.per.connection=1（不需要考虑是否开启幂等性）；</p>
<p>（2）kafka在1.x及以后版本保证数据单分区有序，条件为：</p>
<ul>
<li><p>开启幂等性</p>
<p>max.in.flight.requests.per.connection需要设置小于等于5。</p>
</li>
<li><p>未开启幂等性</p>
<p>max.in.flight.requests.per.connection需要设置为1。</p>
<p>原因说明：因为在kafka1.x以后，启用幂等后，kafka服务端会缓存producer发来的最近5个request的元数据，故无论如何，都可以保证最近5个request的数据都是有序的。</p>
</li>
</ul>
<h2 id="七-Kafka优化"><a href="#七-Kafka优化" class="headerlink" title="七 Kafka优化"></a>七 Kafka优化</h2><h5 id="1、Partition数目"><a href="#1、Partition数目" class="headerlink" title="1、Partition数目"></a>1、Partition数目</h5><ul>
<li>一般来说，每个<code>Partition</code>能处理的吞吐为几MB/s，增加更多的<code>Partition</code>意味着：<ul>
<li>更高的并行度和吞吐</li>
<li>可以扩展更多的（同一个<code>consumer group</code>中的）<code>consumers</code></li>
<li>若是集群中有较多的<code>brokers</code>，则可更大程度上利用闲置的<code>brokers</code></li>
<li>但是会造成<code>Zookeeper</code>的更多选举</li>
<li>也会在<code>Kafka</code>中打开更多的文件</li>
</ul>
</li>
<li>调整规则<ul>
<li>一般来说，若是集群较小（小于个6个<code>broker</code>），则配置<code>2*broker</code>数的<code>partition</code>数。这里主要考虑的是之后的扩展。若是集群扩展了一倍（例如12个），则不用担心会有<code>partition</code>不足的现象发生。</li>
<li>一般来说，若是集群较大（大于12个），则配置<code>1*broker</code>数的<code>partition</code>数。因为这里不需要再考虑集群的扩展情况，与<code>broker</code>数相同的<code>partition</code>数已经足够应付常规场景，若有必要，再手动调整。</li>
<li>考虑最高峰吞吐需要的并行<code>consumer</code>数，调整<code>partition</code>数目。若是应用场景需要有20个（同一个<code>consumer group</code>中的）<code>consumers</code>并行消费，则据此设置为20个<code>partition</code>。</li>
<li>考虑<code>producer</code>所需的吞吐，调整<code>partition</code>数目（如果<code>producer</code>的吞吐非常高，或是在接下来两年内都比较高，则增加<code>partition</code>数目）。</li>
</ul>
</li>
</ul>
<h5 id="2、replication-factor"><a href="#2、replication-factor" class="headerlink" title="2、replication factor"></a>2、replication factor</h5><ul>
<li>此参数决定得到是备份的数目，建议至少设置为2，一般为3，最高设置为4。</li>
<li>更高的<code>replication factor</code>意味着：<ul>
<li>系统更稳定（允许 N - 1 个<code>broker</code>宕机）</li>
<li>更多的副本（如果<code>acks = all</code>，会造成较高的延时）</li>
<li>系统磁盘的使用率会更高（一般若是RF为3，则相对于RF为2时，会占据更多50%的磁盘空间）</li>
</ul>
</li>
<li>调整准则<ul>
<li>以3为起始（当然至少需要有3个<code>brokers</code>，同事也不建议一个kafka集群中节点数量少于3个节点）</li>
<li>如果<code>replication</code>性能成为了瓶颈，则建议使用一个性能更好的<code>broker</code>，而不是降低RF的数目</li>
<li>不要在生产环境中设置RF为1</li>
</ul>
</li>
</ul>
<h5 id="3、批量写入"><a href="#3、批量写入" class="headerlink" title="3、批量写入"></a>3、批量写入</h5><p>为了大幅度提高producer写入吞吐量，需要定期批量写文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每当producer写入10000条消息时，刷数据到磁盘：</span><br><span class="line">log.flush.interval.messages&#x3D;10000</span><br><span class="line"></span><br><span class="line">每间隔一秒，刷数据到磁盘</span><br><span class="line">log.flush.interval.ms&#x3D;1000</span><br></pre></td></tr></table></figure>

<h2 id="八-kafka操作"><a href="#八-kafka操作" class="headerlink" title="八 kafka操作"></a>八 kafka操作</h2><h4 id="1-常用命令"><a href="#1-常用命令" class="headerlink" title="1 常用命令"></a>1 常用命令</h4><p>修改kafka的配置server.properties：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># broker的编号，如果集群汇总有多个broker，则每个broker的编号需求哟设置得不同</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment"># broker对外提供的服务入口地址</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://:9092</span></span><br><span class="line"><span class="comment"># 存放消息日志文件的地址</span></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">D:/kafka_2.12-3.2.0/datas</span></span><br><span class="line"><span class="comment"># kafka所需要的zookeeper集群地址</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">localhost:2181</span></span><br></pre></td></tr></table></figure>

<p>创建topic：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.bat --create --topic test1 --bootstrap-server localhost:9092 --partitions 2 --replication-factor 3</span><br></pre></td></tr></table></figure>

<p>查看topic：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.bat --list --bootstrap-server localhost:9092</span><br></pre></td></tr></table></figure>

<p>查看topic的分区数：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.bat --describe --bootstrap-server localhost:9092 --topic test1</span><br></pre></td></tr></table></figure>

<p>增加分区数（分区数不能减少）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.bat --bootstrap-server localhost:9092 --alter --partitions 7 --topic test1</span><br></pre></td></tr></table></figure>

<p>删除topic：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.bat --bootstrap-server localhost:9092 --delete --topic first</span><br></pre></td></tr></table></figure>

<p>生产者生产消息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer.bat --topic test1 --bootstrap-server localhost:9092</span><br></pre></td></tr></table></figure>

<p>启动消费者接收消息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.b --topic test1 --from-beginning --bootstrap-server localhost:9092</span><br></pre></td></tr></table></figure>

<h4 id="2-在java中使用kafka"><a href="#2-在java中使用kafka" class="headerlink" title="2 在java中使用kafka"></a>2 在java中使用kafka</h4><h5 id="2-1-配置依赖"><a href="#2-1-配置依赖" class="headerlink" title="2.1 配置依赖"></a>2.1 配置依赖</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--        注册中心使用的是Zookeeper，需要引入操作Zookeeper的客户端 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-framework<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="2-2-创建生产者"><a href="#2-2-创建生产者" class="headerlink" title="2.2 创建生产者"></a>2.2 创建生产者</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String brokerList = <span class="string">&quot;localhost:9092&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">&quot;test1&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建kafka 生产者的配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给kafka 配置对象添加配置信息：连接集群bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key,value 序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put( ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put( ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.RETRIES_CONFIG, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用 send 方法 发送消息</span></span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; ++i)&#123;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(topic, <span class="string">&quot;hello&quot;</span> + i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-3-创建消费者"><a href="#2-3-创建消费者" class="headerlink" title="2.3 创建消费者"></a>2.3 创建消费者</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String brokerList = <span class="string">&quot;localhost:9092&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">&quot;test1&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String groupId = <span class="string">&quot;group.demo&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(properties);</span><br><span class="line">        consumer.subscribe(Collections.singleton(topic));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">            <span class="keyword">for</span>(ConsumerRecord&lt;String, String&gt; r: records)&#123;</span><br><span class="line">                System.out.println(r.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-4-常用参数配置"><a href="#2-4-常用参数配置" class="headerlink" title="2.4 常用参数配置"></a>2.4 常用参数配置</h5><ul>
<li>bootstrap.servers：指定生产者客户端连接kafka集群所需要的broker地址清单，具体格式为host1:port1,host2:port2，可以设置一个或多个地址，中间用逗号隔开；</li>
<li>key.serializer和value.serializer：</li>
<li>zookeeper.connect：指明zookeeper主机地址，如果zookeeper是集群，则以逗号隔开，如：172.6.14.61:2181,172.6.14.62:2181</li>
<li>listeners：监听列表，broker对外提供服务时绑定的ip和端口。多个则以逗号隔开。</li>
<li>broker.id：broker的唯一标识符，如果不配置则自动生成，建议配置且一定要保证集群汇总必须唯一</li>
<li>log.dirs：日志数据存放的目录</li>
<li>message.max.bytes：服务器接收单个消息的最大大小，默认为1000012约为976.6KB。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Varrella</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://varrella.github.io/2023/01/29/%E7%AC%94%E8%AE%B0-Kafka/">https://varrella.github.io/2023/01/29/%E7%AC%94%E8%AE%B0-Kafka/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturegg2880x1800-bg-d1ee002.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/01/29/%E7%AC%94%E8%AE%B0-Dubbo/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturewallroom-1920x1080-bg-6335c5b.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Dubbo笔记</div></div></a></div><div class="next-post pull-right"><a href="/2023/01/29/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture1___1-M0WfGD7kI___0___.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info"></div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-Kafka%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">一 Kafka概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">1.1.</span> <span class="toc-text">1 消息队列的好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.2.</span> <span class="toc-text">2 消息队列的两种模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89%E7%82%B9%E5%AF%B9%E7%82%B9%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">1）点对点模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.2.2.</span> <span class="toc-text">2）发布订阅模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-Kafka%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">二 Kafka系统架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Broker"><span class="toc-number">2.1.</span> <span class="toc-text">1 Broker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Topic"><span class="toc-number">2.2.</span> <span class="toc-text">2 Topic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Partition"><span class="toc-number">2.3.</span> <span class="toc-text">3 Partition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Leader"><span class="toc-number">2.4.</span> <span class="toc-text">4 Leader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Producer"><span class="toc-number">2.5.</span> <span class="toc-text">7 Producer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Consumer"><span class="toc-number">2.6.</span> <span class="toc-text">8 Consumer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Consumer-Group"><span class="toc-number">2.7.</span> <span class="toc-text">9 Consumer  Group</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-offset%E5%81%8F%E7%A7%BB%E9%87%8F"><span class="toc-number">2.8.</span> <span class="toc-text">10 offset偏移量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.9.</span> <span class="toc-text">11 元数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-Kafka%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-number">3.</span> <span class="toc-text">三 Kafka生产者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 生产者消息发送流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E7%94%9F%E4%BA%A7%E8%80%85%E5%88%86%E5%8C%BA"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 生产者分区</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E5%88%86%E5%8C%BA%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 分区的好处</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 生产者发送消息的分区策略</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E9%BB%98%E8%AE%A4%E7%9A%84%E5%88%86%E5%8C%BA%E5%99%A8DefaultPartitioner"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">1 默认的分区器DefaultPartitioner</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">2 自定义分区</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E7%94%9F%E4%BA%A7%E8%80%85%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 生产者客户端使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%85%8D%E7%BD%AE%E7%94%9F%E4%BA%A7%E8%80%85%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%82%E6%95%B0%E4%BB%A5%E5%8F%8A%E5%88%9B%E5%BB%BA%E7%9B%B8%E5%BA%94%E7%9A%84%E6%B6%88%E8%B4%B9%E8%80%85%E5%AE%9E%E4%BE%8B"><span class="toc-number">3.3.1.</span> <span class="toc-text">1 配置生产者客户端参数以及创建相应的消费者实例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E5%B9%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E7%94%9F%E4%BA%A7%E8%80%85%E5%AF%B9%E8%B1%A1"><span class="toc-number">3.3.2.</span> <span class="toc-text">2 创建并初始化生产者对象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E5%88%B0broker%E9%9B%86%E7%BE%A4"><span class="toc-number">3.3.3.</span> <span class="toc-text">3 发送数据到broker集群</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">3.3.3.1.</span> <span class="toc-text">3.1 异步发送</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-%E5%90%8C%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">3.3.3.2.</span> <span class="toc-text">3.2 同步发送</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-3-%E5%8F%91%E9%80%81%E6%9D%A1%E4%BB%B6"><span class="toc-number">3.3.3.3.</span> <span class="toc-text">3.3 发送条件</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%85%B3%E9%97%AD%E8%B5%84%E6%BA%90"><span class="toc-number">3.3.4.</span> <span class="toc-text">4 关闭资源</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B-amp-%E6%BA%90%E7%A0%81"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 生产者发送消息核心流程 &amp; 源码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-%E7%94%9F%E4%BA%A7%E8%80%85main%E7%BA%BF%E7%A8%8B%E5%92%8Csender%E7%BA%BF%E7%A8%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.4.1.</span> <span class="toc-text">3.4.1 生产者main线程和sender线程初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-KafkaProducer-%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E6%B5%81%E7%A8%8B-amp-%E6%BA%90%E7%A0%81"><span class="toc-number">3.4.1.1.</span> <span class="toc-text">1 KafkaProducer 构造方法流程 &amp; 源码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-sender%E7%BA%BF%E7%A8%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B-amp-%E6%BA%90%E7%A0%81"><span class="toc-number">3.4.1.2.</span> <span class="toc-text">2 sender线程初始化流程 &amp; 源码</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E5%88%B0%E7%BC%93%E5%86%B2%E9%98%9F%E5%88%97"><span class="toc-number">3.4.2.</span> <span class="toc-text">3.4.2 生产者发送数据到缓冲队列</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-sender%E7%BA%BF%E7%A8%8B%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E5%88%B0kafka%E9%9B%86%E7%BE%A4"><span class="toc-number">3.4.3.</span> <span class="toc-text">3.4.3 sender线程发送数据到kafka集群</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E7%94%9F%E4%BA%A7%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F"><span class="toc-number">3.5.</span> <span class="toc-text">3.5 生产者如何提高吞吐量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-Kafka%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-number">4.</span> <span class="toc-text">四 Kafka消费者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%B6%88%E8%B4%B9%E8%80%85-amp-%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 消费者 &amp; 消费者组</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%88%86%E5%8C%BA%E5%8F%AA%E8%83%BD%E8%A2%AB%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA%E6%B6%88%E8%B4%B9%E8%80%85%E6%B6%88%E8%B4%B9"><span class="toc-number">4.1.1.</span> <span class="toc-text">4.1.1 每一个分区只能被消费者组中的一个消费者消费</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%86%85%E7%9A%84%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%AA%E6%95%B0%E5%8F%98%E5%8C%96%E6%97%B6%E5%AF%B9%E5%BA%94%E7%9A%84%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%9A%84%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.2.</span> <span class="toc-text">4.1.2 消费者组内的消费者个数变化时对应的分区分配的策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.3.</span> <span class="toc-text">4.1.3 分区分配策略</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-RangeAssignor%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.3.1.</span> <span class="toc-text">1  RangeAssignor分配策略</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-RoundRobinAssignor%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.3.2.</span> <span class="toc-text">2 RoundRobinAssignor分配策略</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-StickyAssignor%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.4.</span> <span class="toc-text">3 StickyAssignor分配策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%B6%88%E8%B4%B9%E8%80%85%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 消费者客户端使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E4%BB%A5%E5%8F%8A%E5%88%9B%E5%BB%BA%E5%AE%9E%E4%BE%8B"><span class="toc-number">4.2.1.</span> <span class="toc-text">1 配置参数以及创建实例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%AE%A2%E9%98%85%E4%B8%BB%E9%A2%98"><span class="toc-number">4.2.2.</span> <span class="toc-text">2 订阅主题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF%E5%B9%B6%E6%B6%88%E8%B4%B9"><span class="toc-number">4.2.3.</span> <span class="toc-text">3 拉取消息并消费</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%8F%90%E4%BA%A4%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%A7%BB"><span class="toc-number">4.2.4.</span> <span class="toc-text">4 提交消费位移</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%B6%88%E8%B4%B9%E8%80%85%E5%90%AF%E5%8A%A8%E5%B9%B6%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B-amp-%E6%BA%90%E7%A0%81"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 消费者启动并拉取数据流程 &amp; 源码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">4.3.1.</span> <span class="toc-text">4.3.1 整体流程图</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">1 消费者组初始化流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E8%AF%A6%E7%BB%86%E6%B6%88%E8%B4%B9%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="toc-number">4.3.1.2.</span> <span class="toc-text">2 消费者组详细消费的流程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-KafkaConsumer-%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.3.2.</span> <span class="toc-text">4.3.2 KafkaConsumer 的初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-%E8%AE%A2%E9%98%85%E4%B8%BB%E9%A2%98"><span class="toc-number">4.3.3.</span> <span class="toc-text">4.3.3 订阅主题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-4-%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.3.4.</span> <span class="toc-text">4.3.4 拉取消息的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-updateAssignmentMetadataIfNeeded-%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D"><span class="toc-number">4.3.4.1.</span> <span class="toc-text">1 updateAssignmentMetadataIfNeeded()消费者分区分配</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#ConsumerCoordinator-poll-%E6%B6%88%E8%B4%B9%E8%80%85-%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">4.3.4.1.1.</span> <span class="toc-text">ConsumerCoordinator.poll()消费者&#x2F;消费者组初始化</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-KafkaConsumer-pollForFetches-%E5%BC%80%E5%A7%8B%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">4.3.4.2.</span> <span class="toc-text">2 KafkaConsumer.pollForFetches() 开始拉取数据</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-Kafka%E5%88%86%E5%8C%BA%E5%92%8C%E5%89%AF%E6%9C%AC"><span class="toc-number">5.</span> <span class="toc-text">五 Kafka分区和副本</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E7%9A%84%E5%88%86%E9%85%8D"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 分区副本的分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-kafka%E5%89%AF%E6%9C%AC"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 kafka副本</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Leader-%E7%9A%84%E9%80%89%E4%B8%BE"><span class="toc-number">5.2.0.1.</span> <span class="toc-text">1 Leader 的选举</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Leader%E5%92%8CFollower%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82"><span class="toc-number">5.2.0.2.</span> <span class="toc-text">2 Leader和Follower故障处理细节</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="toc-number">5.3.</span> <span class="toc-text">5.2 Kafka文件存储机制</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-log-%E5%92%8C-index-%E6%96%87%E4%BB%B6"><span class="toc-number">5.3.0.1.</span> <span class="toc-text">1 log 和 index 文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1"><span class="toc-number">5.3.0.2.</span> <span class="toc-text">3 生产者事务</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F-amp-%E4%B9%B1%E5%BA%8F"><span class="toc-number">5.3.0.3.</span> <span class="toc-text">4 数据有序 &amp; 乱序</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-Kafka%E4%BC%98%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">七 Kafka优化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81Partition%E6%95%B0%E7%9B%AE"><span class="toc-number">6.0.0.1.</span> <span class="toc-text">1、Partition数目</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81replication-factor"><span class="toc-number">6.0.0.2.</span> <span class="toc-text">2、replication factor</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E6%89%B9%E9%87%8F%E5%86%99%E5%85%A5"><span class="toc-number">6.0.0.3.</span> <span class="toc-text">3、批量写入</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB-kafka%E6%93%8D%E4%BD%9C"><span class="toc-number">7.</span> <span class="toc-text">八 kafka操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="toc-number">7.0.1.</span> <span class="toc-text">1 常用命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%9C%A8java%E4%B8%AD%E4%BD%BF%E7%94%A8kafka"><span class="toc-number">7.0.2.</span> <span class="toc-text">2 在java中使用kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-%E9%85%8D%E7%BD%AE%E4%BE%9D%E8%B5%96"><span class="toc-number">7.0.2.1.</span> <span class="toc-text">2.1 配置依赖</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-%E5%88%9B%E5%BB%BA%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-number">7.0.2.2.</span> <span class="toc-text">2.2 创建生产者</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-%E5%88%9B%E5%BB%BA%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-number">7.0.2.3.</span> <span class="toc-text">2.3 创建消费者</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-%E5%B8%B8%E7%94%A8%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-number">7.0.2.4.</span> <span class="toc-text">2.4 常用参数配置</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturegg2880x1800-bg-d1ee002.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Varrella</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="https://varrella.github.io/">我始终相信，走过平湖烟雨，岁月山河，那些历尽劫数，尝遍百味的人，会更加生动而干净。</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>