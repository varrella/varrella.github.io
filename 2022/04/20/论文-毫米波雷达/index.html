<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>毫米波雷达点云-论文笔记 | varrella</title><meta name="author" content="Varrella"><meta name="copyright" content="Varrella"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="*《A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection》 2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF). IEEE, 2019，https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.07431 1、">
<meta property="og:type" content="article">
<meta property="og:title" content="毫米波雷达点云-论文笔记">
<meta property="og:url" content="https://varrella.github.io/2022/04/20/%E8%AE%BA%E6%96%87-%E6%AF%AB%E7%B1%B3%E6%B3%A2%E9%9B%B7%E8%BE%BE/index.html">
<meta property="og:site_name" content="varrella">
<meta property="og:description" content="*《A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection》 2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF). IEEE, 2019，https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.07431 1、">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture1___24Pi2uS66-M___0___.jpg">
<meta property="article:published_time" content="2022-04-20T04:17:49.000Z">
<meta property="article:modified_time" content="2022-04-06T06:36:08.000Z">
<meta property="article:author" content="Varrella">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture1___24Pi2uS66-M___0___.jpg"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="https://varrella.github.io/2022/04/20/%E8%AE%BA%E6%96%87-%E6%AF%AB%E7%B1%B3%E6%B3%A2%E9%9B%B7%E8%BE%BE/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-06 14:36:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture微信图片_20210411174101.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">24</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture1___24Pi2uS66-M___0___.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">varrella</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">毫米波雷达点云-论文笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-04-20T04:17:49.000Z" title="Created 2022-04-20 12:17:49">2022-04-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-04-06T06:36:08.000Z" title="Updated 2022-04-06 14:36:08">2022-04-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="毫米波雷达点云-论文笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="《A-Deep-Learning-based-Radar-and-Camera-Sensor-Fusion-Architecture-for-Object-Detection》"><a href="#《A-Deep-Learning-based-Radar-and-Camera-Sensor-Fusion-Architecture-for-Object-Detection》" class="headerlink" title="*《A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection》"></a>*《A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection》</h1><p> 2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF). IEEE, 2019，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.07431">https://arxiv.org/abs/2005.07431</a></p>
<h5 id="1、论文概述"><a href="#1、论文概述" class="headerlink" title="1、论文概述"></a>1、论文概述</h5><p>论文通过在网络层中融合相机数据和雷达稀疏点云投影后的数据，来增强2D目标检测的效果，并提出了一种训练策略BlackIn，将网络训练的重点放在学习雷达数据上。</p>
<h5 id="2、雷达数据预处理"><a href="#2、雷达数据预处理" class="headerlink" title="2、雷达数据预处理"></a>2、雷达数据预处理</h5><p>​    论文中采用的是Continental ARS430毫米波雷达，雷达输出的是2D稀疏点云。</p>
<ul>
<li><p>该模型以图像和雷达数据作为输入，输入的图像包括RGB三通道，另外添加了一维雷达通道。论文中将雷达2D点云数据投影到垂直平面上，雷达数据以像素形式映射到图像平面。</p>
</li>
<li><p>由于雷达不提供目标的高度信息，论文中假设雷达检测到的目标的3D坐标是2D坐标在垂直方向上的延伸，论文中检测的目标分为汽车、卡车、摩托车、自行车和行人，为了覆盖这些目标的高度，假设雷达的检测高度为3m。</p>
</li>
<li><p>为了处理雷达数据的稀疏性，论文中将最后13个雷达周期（约1 s）的数据共同映射到图像上来提高雷达数据的密度，虽然增加部分噪声误差，但该误差在可接受范围内。</p>
</li>
</ul>
<p>示例场景中网络输入数据的格式：</p>
<p>在训练和评估中，论文中对原始数据进行滤波，使得经过过滤的真实数据只包含至少一个雷达检测的目标，滤波后的数据如下：</p>
<p>有一部分目标会被过滤掉，作者在论文中分析了几点原因：论文中不补偿其他目标的运动，雷达和相机的空间标定会存在偏差、雷达和相机的时间对齐等。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20210930155150.png"></p>
<h5 id="3、网络结构"><a href="#3、网络结构" class="headerlink" title="3、网络结构"></a>3、网络结构</h5><p>论文的网络模型建立在RetinaNet的基础上，以VGG16作为backbone，网络的输入为增加了雷达通道的四维图像，通过VGG处理雷达和相机的数据。</p>
<p>左侧的分支中，原始雷达数据经过max-pooling层以不同的比例处理雷达数据输入；</p>
<p>以VGG16的卷积块作为主要处理单元，在每层均进行融合处理，深层网络增加FPN（特征金字塔）产生分类和回归结果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20210930155336.png"></p>
<p>论文中提出了一种雷达和相机融合训练的策略，借鉴了Dropout的思想，论文中称之为BlackIn。对于百分之20的训练图像，在随机的训练步长中，丢弃所有用于图像特征提取的神经元，目的在于图像数据的缺失使得网络能更多地依靠雷达数据的特征。</p>
<p>预训练模型采用的是在图像上训练的模型。</p>
<h5 id="4、论文中的实验结果"><a href="#4、论文中的实验结果" class="headerlink" title="4、论文中的实验结果"></a>4、论文中的实验结果</h5><ul>
<li><p>数据集：nuScenes、TUM（论文中制作的数据集）</p>
<p>数据集中目标的数量如图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20210930160724.png"></p>
</li>
<li><p>baseline：RetinaNet</p>
</li>
<li><p>论文中的实验对比图，对比了baseline和论文中的方法，以及论文中提出的BlackIn策略、几种毫米波雷达数据滤波方法。</p>
</li>
</ul>
<p>【备注】这几种过滤方式没太看懂，论文中也没有详细解释，也没查到相关资料。该部分论文原文：</p>
<h5 id="5、论文复现"><a href="#5、论文复现" class="headerlink" title="5、论文复现"></a>5、论文复现</h5><p>【官方代码】<a target="_blank" rel="noopener" href="https://github.com/TUMFTM/CameraRadarFusionNet">https://github.com/TUMFTM/CameraRadarFusionNet</a></p>
<p>nuscenes的mini数据集训练了60个epoc的测试结果：</p>
<p>验证集上的评估该模型，横坐标表示召回率，纵坐标表示精度：</p>
<p>没有自己的数据集，暂无法测试。</p>
<h1 id="《CenterFusion-Center-based-Radar-and-Camera-Fusion-for-3D-Object-Detection》（WACV2021）"><a href="#《CenterFusion-Center-based-Radar-and-Camera-Fusion-for-3D-Object-Detection》（WACV2021）" class="headerlink" title="*《CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection》（WACV2021）"></a>*《CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection》（WACV2021）</h1><p><a target="_blank" rel="noopener" href="https://github.com/mrnabati/CenterFusion">https://github.com/mrnabati/CenterFusion</a></p>
<p><a target="_blank" rel="noopener" href="https://crossminds.ai/video/centerfusion-center-based-radar-and-camera-fusion-for-3d-object-detection-600f910d6b1b6888a6332a5a/">https://crossminds.ai/video/centerfusion-center-based-radar-and-camera-fusion-for-3d-object-detection-600f910d6b1b6888a6332a5a/</a></p>
<h5 id="1、论文概述-1"><a href="#1、论文概述-1" class="headerlink" title="1、论文概述"></a>1、论文概述</h5><p>论文提出了一种利用雷达和相机进行3D目标检测的融合方法。首先使用CenterNet（ Xingyi Zhou, Dequan Wang, and Philipp Kr¨ahenb¨uhl. Objects as points. arXiv preprint arXiv:1904.07850, 2019）作为目标检测网络预测图像中目标的中心点。然后提出一种新的基于视锥的方法来解决数据关联问题，将雷达探测到的目标与其对应的目标中心点关联起来。通过毫米波雷达和图像融合后的特征，回归出目标的深度、旋转和速度等信息。</p>
<h5 id="2、雷达数据预处理-1"><a href="#2、雷达数据预处理-1" class="headerlink" title="2、雷达数据预处理"></a>2、雷达数据预处理</h5><p>为了解决毫米波雷达高度信息不准确的问题，引入了雷达点云预处理步骤，称为pillar expansion，将每个雷达点云扩展为一个固定大小的3D pillar，将这些雷达pillar直接映射到图像中（中间的图）会出现重叠的问题，采用论文中提出的视锥关联机制（最下面的图）能减少重叠的情况。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005173848.png"></p>
<h5 id="3、网络结构-1"><a href="#3、网络结构-1" class="headerlink" title="3、网络结构"></a>3、网络结构</h5><p>利用关键点检测网络生成初步的三维检测，并提出一种新的基于截锥的雷达关联方法，精确地将雷达检测与三维空间中相应的目标进行关联。这些雷达探测然后映射到图像平面，并用于创建特征图，以补充基于图像的特征。最后，利用融合后的特征精确估计物体的深度、旋转和速度等三维属性。</p>
<ul>
<li>Backbone：论文采用修改后的CenterNet作为目标检测网络，对图像进行初步检测，预测目标的中心点。采用不同的回归模块，回归其他目标属性，例如物体的三维坐标、中心偏移、深度、2D大小（宽高）等。</li>
<li>毫米波雷达数据进行预处理之后，论文提出了一种融合机制（视锥关联机制Frustum Association），将毫米波雷达探测到的目标与其对应的目标中心点进行关联，将雷达数据映射到图像中，得到融合后的特征。</li>
<li>融合后的特征进行第二次回归，重新估计它们的深度、速度等信息来改进初始检测结果，生成3D边界框。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005172155.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005173613.png"></p>
<p>【视锥关联方法】利用物体的二维边界框及其估计的深度和尺寸来创建物体的三维感兴趣区域(RoI)视锥。为一个对象创建一个精确的2D边界框，我们为该对象创建一个锥视图。这大大缩小了需要检查关联的雷达探测范围，因为此截锥外的任何点都可以忽略。然后，使用估计的目标深度、维数和旋转来创建目标周围的RoI，进一步过滤掉与该目标不相关的雷达检测。如果该RoI内存在多个雷达探测点，则取最近的点作为该目标对应的雷达探测点。</p>
<p>【融合机制】融合机制的关键是雷达探测与目标的精确关联。中心点目标检测网络为图像中的每个目标类别生成一张热图。热图中的峰值代表了目标可能的中心点，这些位置的图像特征被用来估计目标的其他属性。在这种设置下，为了利用雷达信息，需要将基于雷达的特征映射到图像上对应目标的中心，这就要求雷达探测与场景中的目标之间有准确的关联。</p>
<h5 id="4、论文中的实验结果-1"><a href="#4、论文中的实验结果-1" class="headerlink" title="4、论文中的实验结果"></a>4、论文中的实验结果</h5><p>论文对比了基于相机的3D目标检测方法：OFT、MonoDIS、CenterNet以及基于激光雷达的3D目标检测方法：InfoFocus，在nuScenes数据集的测试机和验证集上的测试结果。 </p>
<p>（mATE、mASE、mAOE、mAVE、mAAE、NDS是nuScenes数据集的评估标准。NDS is a weighted sum between mAP, the mean average errors of location/translation (mATE), size/scale (mASE), orientation (mAOE), attribute (mAAE), and velocity (mAVE).）</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005172239.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005172310.png"></p>
<p>在BEV图（鸟瞰图）中，蓝色是预测值，红色是真实值，绿色是毫米波点云。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005172329.png"></p>
<h5 id="5、复现结果"><a href="#5、复现结果" class="headerlink" title="5、复现结果"></a>5、复现结果</h5><p>该论文代码开源（<a target="_blank" rel="noopener" href="https://github.com/mrnabati/CenterFusion%EF%BC%89%EF%BC%8C%E5%9C%A8nuScenes">https://github.com/mrnabati/CenterFusion），在nuScenes</a> mini数据集上训练了60个epoc并在验证集上进行测试的评估结果。可视化结果需修改源码，暂无可视化结果。</p>
<h1 id="《Radar-Camera-Sensor-Fusion-for-Joint-Object-Detection-and-Distance-Estimation-in-Autonomous-Vehicles》"><a href="#《Radar-Camera-Sensor-Fusion-for-Joint-Object-Detection-and-Distance-Estimation-in-Autonomous-Vehicles》" class="headerlink" title="*《Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles》"></a>*《Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles》</h1><p>12th Workshop on Planning, Perception and Navigation for Intelligent Vehicles, IROS 2020，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.08428">https://arxiv.org/abs/2009.08428</a></p>
<h5 id="1、论文概述-2"><a href="#1、论文概述-2" class="headerlink" title="1、论文概述"></a>1、论文概述</h5><p>论文提出了一种融合毫米波雷达点云和RGB图像的中间融合方法，先从雷达点云中生成一组3D Boxes，然后映射到图像中。对图像进行卷积操作提取图像特征，将图像特征和雷达点云3D Box映射到图像后的信息同时输入到网络中进行定位和分类。</p>
<p>文中提出的方法是一种两阶段的没目标检测方法，融合了雷达点云和图像特征来生成目标候选区（proposal），对于每一个目标候选区估计一个距离值（distance from the vehicle），然后将这个目标候选区输入到第二阶段的预测分类中。</p>
<h5 id="2、网络结构"><a href="#2、网络结构" class="headerlink" title="2、网络结构"></a>2、网络结构</h5><p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004105656.png"></p>
<p>（1）输入：以雷达点云数据和图像数据作为输入，每部分数据先单独处理。采用了基于Region Proposal的两阶段目标检测方法。雷达分支生成3D候选框，然后映射到图像中，结合图像分支提取的特征来提高定位的准确性。</p>
<p>（2）雷达分支</p>
<ul>
<li>将每一个雷达点云作为一个检测到的目标，使用数据集中每个类别预定义的3D anchors，直接生成3D object proposal，并附有目标距离信息。【3D anchors表示成(x, y, z, w, l, h, r)，(x, y, z)表示中心点的坐标，由雷达测得的坐标直接获得，(w, l, h)表示anchors 的大小，每一个类别是固定值，r是这个3D box基于车辆坐标系的方位角（0°和90°）。对于每一个雷达点云生成2n个boxes，n为数据集中目标的类别个数】</li>
<li>将3D anchors映射到图像中，通过为每个映射锚点寻找最小的包围盒来转换成等价的2D bounding boxes。2D bounding box的建议距离即为3D bounding box中雷达检测到的距离。</li>
<li>将生成的2D proposals输入到Radar Proposal Refinement（RPR）子网络中，这个网络融合了雷达信息和图像信息来提高定位的精度。<ul>
<li>由于雷达检测到的目标并不总是以图像上的对应目标为中心，因此生成的3D anchors 和2D proposals会有一定的偏移。RPR中的Box Regressor层使用每个雷达候选区中的图像特征来对偏移量进行回归。</li>
<li>RPR中的Box Classifier层为每一个雷达候选框进行分类，计算得分，用于区分真正的目标和背景。</li>
<li>RPR中的分类和回归层的输入是雷达提取的候选区域（radar proposal）中的图像特征，对每个radar proposal进行分类和回归。</li>
<li>由于雷达候选框大小由于距离原因存在大小不一致，因此在RPR中先进性一次ROI Pooling，将大小调整相同。</li>
</ul>
</li>
</ul>
<p>（2）图像分支</p>
<ul>
<li>图像先经过CNN提取特征，然后输入到一个RPN（Region Proposal Network）网络生成proposal。在RPN层中添加了1*1的全连接距离回归层来预测目标候选框的距离。（距离的产生有两种方式：雷达直接获取，图像回归）</li>
<li>使用交叉熵损失进行分类，以及L1平滑进行回归。</li>
</ul>
<p>（3）第二阶段：融合</p>
<p>计算雷达proposal和图像proposal之间的IOU（交并比），设置一个阈值进行筛选，这些筛选出来的proposals的距离信息采用雷达proposals中的距离信息，而非图像回归出来的距离信息。</p>
<p>然后使用NMS去除多余的proposals，然后输入到目标检测网络进行分类。输出边界框回归和分类的结果，包含目标距离信息。通过计算雷达proposal和图像proposal之间的IOU来寻找匹配的候选框，讲这些匹配的候选框中雷达检测到的距离覆盖掉图像回归出来的距离。剩下的proposals被送入第二阶段网络进行分类和计算得分。</p>
<p>这个网络的结构类似于FAST RCNN，特征图输入到池化层中将所有特征图的大小调整至一致，然后通过一系列全连接层，然后输入到softmax和bbox回归层。最终输出的是目标的类别和所在的包围框，以及目标的距离。</p>
<p>（3）损失函数</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004115449.png"></p>
<h5 id="3、论文中的实验结果"><a href="#3、论文中的实验结果" class="headerlink" title="3、论文中的实验结果"></a>3、论文中的实验结果</h5><p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004115803.png"></p>
<p>AR：Average Recall</p>
<p>MAE： Mean Absolute Error （平均绝对误差，衡量距离估计精度）</p>
<p>对比了CRF-Net和RRPN（Rrpn: Radar region proposal network for object detection in autonomous vehicles, 2019 IEEE International Conference on Image Processing (ICIP)）中的方法。</p>
<p>论文中的方法较这两种方法（RRPN《Rrpn: Radar region proposal network for<br>object detection in autonomous vehicles》、CRF-Net《A<br>deep learning-based radar and camera sensor fusion architecture for<br>object detection》）精度有所提高。</p>
<h1 id="《Rrpn-Radar-region-proposal-network-for-object-detection-in-autonomous-vehicles》"><a href="#《Rrpn-Radar-region-proposal-network-for-object-detection-in-autonomous-vehicles》" class="headerlink" title="《Rrpn: Radar region proposal network for object detection in autonomous vehicles》"></a>《Rrpn: Radar region proposal network for object detection in autonomous vehicles》</h1><p>（ICIP 2019）</p>
<h5 id="1、论文概述-3"><a href="#1、论文概述-3" class="headerlink" title="1、论文概述"></a>1、论文概述</h5><p>论文中提出了一种利用毫米波雷达点云信息在图像中生成候选区域的算法，可以适用于两阶段目标检测方法，可以在和毫米波雷达和图像数据进行融合的时候作为参考。</p>
<p><strong>基于Fast-RCNN，采用两种不同的骨干网络：ResNet-101网络和ResNeXt-101，将nuscenes数据集转换成coco数据集格式，先在coco数据集上进行预训练，然后在转换成coco格式的nuscenes数据集上进行fine-tune。</strong></p>
<h5 id="2、基于毫米波雷达检测的RRPN"><a href="#2、基于毫米波雷达检测的RRPN" class="headerlink" title="2、基于毫米波雷达检测的RRPN"></a>2、基于毫米波雷达检测的RRPN</h5><p>包含以下三个步骤：</p>
<p>（1）坐标转换（perspective transformation）</p>
<p>将毫米波雷达检测到的目标点坐标，通过标定参数从车辆坐标中转换到图像坐标中。转换矩阵通过标定参数决定。</p>
<p>（2）锚点生成 （anchor generation）</p>
<p>将毫米波雷达检测到的目标的坐标映射到图像中，则不需要对图像做额外的处理。</p>
<p>存在的问题是：毫米波雷达检测到的目标点并不总是在目标的中心位置，还有可能在目标的边界上，因此对于每一个雷达点生成以雷达检测点为中心的4种不同大小和3种不同长宽比的anchor box。</p>
<p>另一个问题是雷达并没有提供目标的大小信息，所以对不同大小的目标提出一个固定大小的边界框是不准确的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004155244.png"></p>
<p>（3）距离补偿（distance compensation）</p>
<p>毫米波雷达能够检测到目标的距离信息，不同距离的目标在图像上显示的大小不同，论文中根据目标的距离信息，对生成的anchor box根据不同的尺度因子进行缩放，进一步提高准确性。</p>
<p>尺度因子Si，α和β通过最大化生成的包围框和真实值的交并比来学习得到：</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004155715.png"></p>
<h5 id="3、论文中的实验结果-1"><a href="#3、论文中的实验结果-1" class="headerlink" title="3、论文中的实验结果"></a>3、论文中的实验结果</h5><p>在Nuscenes数据集上进行测试，将Nuscenes数据集分为两部分，第一部分包括车前雷达和相机的数据，第二部分包括车后雷达和相机的数据。采用文中提出的区域生成网络RRPN以及另一种候选区域生成算法Selective Search（《Selective Search for Object Recognition》），分别采用ResNet-101 和ResNeXt-101（2017CVPR《Aggregated Residual Transformations for Deep Neural Networks》）作为backbone进行对比。</p>
<p>论文中的结果如下图所示：文中提出的RRPN算法在大部分类别上优于Selective Search算法。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004160724.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004160743.png"></p>
<h5 id="4、论文复现"><a href="#4、论文复现" class="headerlink" title="4、论文复现"></a>4、论文复现</h5><p>论文中的代码开源（ <a target="_blank" rel="noopener" href="https://github.com/mrnabati/RRPN">https://github.com/mrnabati/RRPN</a> ）</p>
<p>配置运行环境，并在Nuscenes-mini数据集上进行训练，训练完成之后，在运行测试代码的时候出现问题，还未解决。</p>
<h1 id="《2D-Car-Detection-in-Radar-Data-with-PointNets》（2019-IEEE-Intelligent-Transportation-Systems-Conference-ITSC-）"><a href="#《2D-Car-Detection-in-Radar-Data-with-PointNets》（2019-IEEE-Intelligent-Transportation-Systems-Conference-ITSC-）" class="headerlink" title="《2D Car Detection in Radar Data with PointNets》（2019 IEEE Intelligent Transportation Systems Conference (ITSC)）"></a>《2D Car Detection in Radar Data with PointNets》（2019 IEEE Intelligent Transportation Systems Conference (ITSC)）</h1><h5 id="1、论文概述-4"><a href="#1、论文概述-4" class="headerlink" title="1、论文概述"></a>1、论文概述</h5><p>论文中借鉴了PointNet和Frustum PointNets的思想，在PointNet的基础上，仅使用毫米波雷达的数据实现2D目标分类和分割。论文中采用了自己制作的数据集（仅包含car一个类别）进行评估。</p>
<h5 id="2、网络结构-1"><a href="#2、网络结构-1" class="headerlink" title="2、网络结构"></a>2、网络结构</h5><p>网络结构是基于PointNet。包含三个主要部分。</p>
<p>输入数据是毫米波雷达点云点云，包含四个维度的数据，分别是x坐标、y坐标、径向速度和目标的横截面积。</p>
<p>（1）Patch Proposal：该模块将输入的雷达目标的点云数据划分成多个特定长宽的感兴趣区域patch，每个patch中都包含雷达点云。</p>
<p>（2）Classification and Segmentation：该模块包含一个分类网络和一个分割网络。分类网络对每个patch进行分类，来区分类别是否是car。分割模块对于类别是car的patch进行分割，并预测一个概率值。在Masking步骤中对提取出类别是car的雷达目标。</p>
<p>（3）2D Bounding Box Estimation：该模块利用分割后的结果，采用Transformer PointNet(T-Net)预测出2D边界框的中心点。然后采用2D Bounding Box Estimation PointNet回归出边界框，包含x、y坐标、角度和长宽。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004210627.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211004210707.png"></p>
<h5 id="3、论文中的实验结果-2"><a href="#3、论文中的实验结果-2" class="headerlink" title="3、论文中的实验结果"></a>3、论文中的实验结果</h5><p>论文中制作了一个数据集，ego vehicle代表安装了毫米波雷达的车，target vehicle代表目标车辆，每个雷达周期只有一个目标，采集的数据实在不同的场景以及两辆车不同的的相对位置下采集的，在数据集上进行了测试。整体精度还行，但是数据集比较有限。</p>
<p>红色点表示car，蓝色点表示非车辆目标，红色的框是生成的边界框。</p>
<h1 id="《RADAR-RGB-attentive-fusion-for-robust-object-detection-in-autonomous-vehicles》（ICIP-2020）"><a href="#《RADAR-RGB-attentive-fusion-for-robust-object-detection-in-autonomous-vehicles》（ICIP-2020）" class="headerlink" title="《RADAR+RGB attentive fusion for robust object detection in autonomous vehicles》（ICIP 2020）"></a>《RADAR+RGB attentive fusion for robust object detection in autonomous vehicles》（ICIP 2020）</h1><h5 id="1、论文概述-5"><a href="#1、论文概述-5" class="headerlink" title="1、论文概述"></a>1、论文概述</h5><p>论文中提出了BIRANet方法，采用两阶段的目标检测方法，先分别对毫米波雷达数据和图像数据进行特征提取，在RPN过程中使用了基于毫米波和图像分别生成anchors，提出了最终anchors的选择方法，生成最终的候选框。</p>
<h5 id="2、网络结构-2"><a href="#2、网络结构-2" class="headerlink" title="2、网络结构"></a>2、网络结构</h5><p>（1）毫米波雷达数据特征提取</p>
<p>先将毫米波雷达检测到的目标的坐标转换到图像坐标系中，然后将雷达检测到的点调整成和图像一样大小的特征图，其中每个点的x、y坐标即为雷达检测到的坐标，z坐标为雷达检测到的目标到雷达的距离，没有雷达点的部分均设为0，然后输入到残差网络中进行特征提取。</p>
<p><img src="C:\Users\11951\AppData\Roaming\Typora\typora-user-images\image-20211005141753275.png" alt="image-20211005141753275"></p>
<p>（2）图像特征提取</p>
<p>论文中采用ResNet对图像进行特征提取。</p>
<p>（3）毫米波和图像特征融合</p>
<p>融合时，对雷达特征提取分支的输出特征图和RGB特征提取分支的第二阶段输出特征图进行元素加法运算。论文中提到，通过大量实验证明，将毫米波雷达数据特征提取的结果和图像特征提取网络中的第二层进行融合，能取得更好的结果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005144135.png"></p>
<p>在提取特征图P2-P5的过程中，论文中提出在毫米波雷达数据特征提取中用到了scSE（Spatial-Channel Sequeeze &amp; Excitation：cSE（研究feature map的通道重要性）和sSE（研究feature map的空间重要性）的结合 ）模块来增强雷达特征提取的效果，突出了雷达点所在区域的通道信息和空间特征。</p>
<p>（3）RPN</p>
<p>雷达点不仅出现在物体表面的中心，而且也出现在物体的上、下、左、右边缘。因此，在一个好的建议方案中，不能只考虑边界盒中心的雷达点，还应该考虑边界盒所有四条边上的雷达点。锚盒在每个雷达点按3个比率(0.5,1.0,2.0)生成，并考虑每个雷达点位于锚盒的上、下、左、右边缘和中心(TDLRC)。在RPN中，雷达点被映射到FPN的每个输出特征图上。根据特征图的层次，采用不同比例尺提取每个雷达点的锚点。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005144051.png"></p>
<p>从雷达数据生成的特征图中生成anchors，以及从图像数据中生成anchors，并和ground truth进行比较，选择最终的anchors：</p>
<p>用ROI Align方法代替Faster RCNN中的ROI pooling层，生成最终边界框。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005145036.png"></p>
<h5 id="3、论文中的检测结果"><a href="#3、论文中的检测结果" class="headerlink" title="3、论文中的检测结果"></a>3、论文中的检测结果</h5><p>FFPN：基于FPN的Faster RCNN</p>
<p>RANet（Radar）：RPN过程中仅使用雷达数据生成的anchors</p>
<p>BIRANet（RGB+Radar)：RPN过程中使用上述算法选择的anchors</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005150358.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005150453.png"></p>
<h5 id="4、论文复现-1"><a href="#4、论文复现-1" class="headerlink" title="4、论文复现"></a>4、论文复现</h5><p>论文中代码开源，但在复现的过程中出现了问题，在作者的github上有人提出了相同的问题，但是还没有解决方案。提供的预训练模型权重的维度有点问题。</p>
<h1 id="《Distant-Vehicle-Detection-Using-Radar-and-Vision》（2019-International-Conference-on-Robotics-and-Automation-ICRA-）"><a href="#《Distant-Vehicle-Detection-Using-Radar-and-Vision》（2019-International-Conference-on-Robotics-and-Automation-ICRA-）" class="headerlink" title="《Distant Vehicle Detection Using Radar and Vision》（2019 International Conference on Robotics and Automation (ICRA)）"></a>《Distant Vehicle Detection Using Radar and Vision》（2019 International Conference on Robotics and Automation (ICRA)）</h1><h5 id="1、论文概述-6"><a href="#1、论文概述-6" class="headerlink" title="1、论文概述"></a>1、论文概述</h5><p>论文中采用毫米波雷达和相机融合进行目标检测的方法，旨在检测远处的小目标。论文中介绍了一种通过组合来自多个相机的检测结果来自动标记新数据集的方法，采用concat和element-wise 两种方法对图像数据和雷达数据进行特征级融合，文中实验结果表明融合了雷达数据的目标检测在检测小目标上比单纯使用图像目标检测精度有所提高。</p>
<h5 id="2、数据采集和预处理"><a href="#2、数据采集和预处理" class="headerlink" title="2、数据采集和预处理"></a>2、数据采集和预处理</h5><p>（1）图像数据预处理</p>
<p>论文中使用了一个长焦镜头和一个短焦镜头进行图像采集。毫米波雷达数据采用ESR德尔福雷达进行采集</p>
<p>对数据进行标注的方法采用自动标注和手动标准相结合：首先对两个相机采集到的图像使用在KATTI数据集上进行训练的YOLO模型进行检测，然后将两个相机采集的图像进行坐标系统一，图像重叠的部分选择长焦镜头拍摄的图像检测后的结果作为ground truth。并采用了翻转、调整色调和饱和度、裁剪的方法进行图像增强。</p>
<p>（2）雷达数据预处理</p>
<p>雷达检测到的目标点，将每个目标的径向速度和距离投影到图像中，作为图像的两个额外的通道。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005160818.png"></p>
<p>（3）由于行人的雷达数据较差，在实验过程中主要关注车辆的检测。</p>
<h5 id="3、网络结构-2"><a href="#3、网络结构-2" class="headerlink" title="3、网络结构"></a>3、网络结构</h5><p>简历在SSD的基础上，包含RGB分支和雷达两个分支，采用7*7卷积层、最大池化层和ResNet18进行特征提取。分别提取图像特征和雷达数据特征。</p>
<p>并分别采用concat和element-wise 两种方法，在第二个图像ResNet块中对图像数据和雷达数据进行特征级融合。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005154555.png"></p>
<h5 id="4、论文中的实验结果-2"><a href="#4、论文中的实验结果-2" class="headerlink" title="4、论文中的实验结果"></a>4、论文中的实验结果</h5><p>采用上面的网络结构，RGB only代表上面的网络结构中仅仅保留RGB分支，（RGB+Radar，concatenation）代表采用concat方法对图像特征和雷达特征进行融合，（RGB+Radar，element-wise）代表采用element-wise方法对图像特征和雷达特征进行融合。从论文中的结果来看，采用毫米波雷达和图像融合的方法用于小目标检测比单纯使用图像检测有显著提高。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005160924.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005160854.png"></p>
<h1 id="《Radar-Camera-Fusion-via-Representation-Learning-in-Autonomous-Driving》-（CVPR-2021"><a href="#《Radar-Camera-Fusion-via-Representation-Learning-in-Autonomous-Driving》-（CVPR-2021" class="headerlink" title="《Radar Camera Fusion via Representation Learning in Autonomous Driving》 （CVPR 2021"></a>《Radar Camera Fusion via Representation Learning in Autonomous Driving》 （CVPR 2021</h1><p> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.07825%EF%BC%89">https://arxiv.org/abs/2103.07825）</a></p>
<h5 id="1、论文概述-7"><a href="#1、论文概述-7" class="headerlink" title="1、论文概述"></a><strong>1</strong>、<strong>论文概述</strong></h5><p>论文中提出了一种基于表示学习(representation learning)的毫米波雷达和图像融合的方法，对毫米波雷达的检测结果和图像的检测结果，和原图像一起输入到模型中进行特征提取，最后找出最有相关性的雷达检测结果和图像检测结果。</p>
<h5 id="2、网络结构-3"><a href="#2、网络结构-3" class="headerlink" title="2、网络结构"></a><strong>2</strong>、<strong>网络结构</strong></h5><p>【预处理】在预处理阶段进行时间和空间的对齐。对于每帧图像寻找最近的那帧雷达数据来对齐。时间对齐后，将毫米波雷达坐标转换到图像上。每个图像先输入目标检测网络中，生成对应的目标包围框，包围框中的属性如下图所示。预处理后，将雷达检测点与图像检测的包围框进行关联。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005162631.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005162705.png"></p>
<p>【网络结构】</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005164054.png"></p>
<p>【Process a】：输入毫米波雷达的检测结果（radar pin）和图像的检测结果（2D bounding box），分别转换到图像中不同的通道中，以像素点的形式表示，图像通道的像素点表示bounding box的中心点，毫米波雷达通道的像素点表示毫米波雷达检测到的目标映射到图像中的坐标。</p>
<p>【Process b】：将上一步骤中生成的伪图像和原来的RGB图像进行拼接，生成最终的伪图像（pseudo-image）。每个属性占一个通道，这个伪图像包含七个雷达通道的信息（目标ID、目标存在的概率、x坐标、y坐标、x方向速度、y方向速度、一个表示投影之后像素坐标热图heatmap）和四个bounding box通道的信息（宽度、高度、目标类别、一个表示）以及三个RGB图像通道的信息。然后使用AssociationNet进行学习。</p>
<p>【Neural Network】：将上一步骤中生成的伪图像输入到网络中，其中以ResNet-50作为backbone，后面跟了特征金字塔，以及两个额外的层将输出的特征图大小恢复到原始的输入大小。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005165135.png"></p>
<p>【Process c】：Neural Network的最后一层用来提取出 一个表示向量representation vectors 。由于每个radar pin 和图像bounding box在featuremap中都有一个唯一的像素位置，process c中提取每个radar pin或图像bounding box在输出featuremap中对应像素位置的表示向量（representation vector）。</p>
<p>【损失函数】：如果一个radar pin和一个bounding box都来自同一个目标则视为正样本，否则视为负样本。最后通过最小化正样本（radar pin和bounding box指向同一个目标）的representation vectors之间的欧式距离和最大化负样本的representation vectors之间的欧式距离来设计损失函数，找到最有关联的radar pin和bounding box对作为融合结果。</p>
<p>【正样本的损失函数】：<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005170218.png"></p>
<p>【负样本的损失函数】：<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005170311.png"></p>
<p>【序列损失】：AssociationNet所犯的一种特殊错误是，它可能违反了简单的顺序规则，即给定两对相关的雷达引脚和包围盒，距离更远的雷达引脚与距离更近的包围盒相关联。为了解决这个问题，引入了序损。解决远处的radar pin和近处的bounding box匹配错误的问题。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005171139.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005171336.png"></p>
<p>【总的损失函数】：<img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005171233.png"></p>
<p>其中，POS和NEG表示正负样本集，npos和nneg表示正负样本集中的关联总数，（i1，i2）表示radar pin i1和bounding box i2组成的第i个关联对，h1和h2表示representation vectors，m1和m2是正关联和负关联之间的距离阈值。Word是可调节的常量。</p>
<p><img src="C:\Users\11951\AppData\Roaming\Typora\typora-user-images\image-20211005171540902.png" alt="image-20211005171540902"></p>
<p>【推理】在推理过程中，计算所有可能的radar pin和bounding-box对之间，特征向量的欧氏距离。如果距离低于一定的阈值，雷达引脚和包围盒将被认为是一个成功的关联。<code>During inference, we calculate the Euclidean distance between the representation vectors of all possible radar-pin-bounding-box pairs. If the distance falls below a certain threshold, the radar pin and the bounding box will be considered as a successful association. </code></p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005173910.png"></p>
<h5 id="3、论文中的检测结果-1"><a href="#3、论文中的检测结果-1" class="headerlink" title="3、论文中的检测结果"></a><strong>3、论文中的检测结果</strong></h5><p>使用的是作者自制的数据集。AssociationNet是在一个由测试车队收集的12个驾驶序列的内部数据集上进行训练和评估的，该数据集包括14.8小时在各种驾驶场景下的驾驶，包括高速公路、城市和城市道路。为了降低相邻帧之间的时间相关性，雷达和相机初始同步频率为10  Hz，然后进一步向下采样至2  Hz。12个序列中的11个用于训练，另一个用于测试。因此，在训练数据集中有10414帧同步雷达和相机，在测试数据集中有2714帧同步雷达和相机。对于训练数据，采用传统的基于规则的算法生成关联标签，并附加过滤以提高精度。对于测试数据，我们使用人工注释器手动管理标签，以获得高质量的真实标签。</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005171421.png"></p>
<p>对比了传统方法（融合规则手工定制，基于一些启发式规则或一定的距离度量）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/20211005171505.png"></p>
<h1 id="PCD版本"><a href="#PCD版本" class="headerlink" title="PCD版本"></a>PCD版本</h1><p>在点云库（PCL）1.0版本发布之前，PCD文件格式有不同的修订号。这些修订号用PCD_Vx来编号（例如，PCD_V5、PCD_V6、PCD_V7等等），代表PCD文件的0.x版本号。然而PCL中PCD文件格式的正式发布是0.7版本（PCD_V7）。</p>
<h3 id="文件头格式"><a href="#文件头格式" class="headerlink" title="文件头格式"></a>文件头格式</h3><p>每一个PCD文件包含一个文件头，它确定和声明文件中存储的点云数据的某种特性。PCD文件头必须用ASCII码来编码。PCD文件中指定的每一个文件头字段以及ascii点数据都用一个新行（\n）分开了，从0.7版本开始，PCD文件头包含下面的字段：</p>
<ul>
<li><p><strong>VERSION –指定PCD文件版本</strong></p>
</li>
<li><p><strong>FIELDS –指定一个点可以有的每一个维度和字段的名字</strong>。例如：</p>
<ul>
<li>FIELDS x y z                  # XYZ data</li>
<li>FIELDS x y z rgb             # XYZ + colors</li>
<li>FIELDS x y z normal_xnormal_y normal_z     # XYZ + surface normals</li>
<li>FIELDS j1 j2 j3                # moment invariants</li>
<li>…</li>
</ul>
</li>
<li><p><strong>SIZE –用字节数指定每一个维度的大小</strong>。例如：</p>
<ul>
<li><em>unsigned char</em>/<em>char</em> has 1 byte</li>
<li><em>unsigned short</em>/<em>short</em> has 2 bytes</li>
<li><em>unsignedint</em>/<em>int</em>/<em>float</em> has 4 bytes</li>
<li><em>double</em> has 8 bytes</li>
</ul>
</li>
<li><p><strong>TYPE –用一个字符指定每一个维度的类型</strong>。现在被接受的类型有：</p>
<ul>
<li><strong>I</strong> –表示有符号类型int8（char）、int16（short）和int32（int）</li>
<li><strong>U</strong> – 表示无符号类型uint8（unsigned char）、uint16（unsigned short）和uint32（unsigned int）</li>
<li><strong>F</strong> –表示浮点类型</li>
</ul>
</li>
<li><p><strong>COUNT –指定每一个维度包含的元素数目</strong>。</p>
<p>例如，x这个数据通常有一个元素，但是像VFH这样的特征描述子就有308个。实际上这是在给每一点引入n维直方图描述符的方法，把它们当做单个的连续存储块。默认情况下，如果没有COUNT，所有维度的数目被设置成1。</p>
</li>
<li><p><strong>WIDTH –用点的数量表示点云数据集的宽度。</strong>根据是有序点云还是无序点云，WIDTH有两层解释：</p>
<ul>
<li><p>它能确定无序数据集的点云中点的个数（和下面的POINTS一样）</p>
</li>
<li><p>它能确定有序点云数据集的宽度（一行中点的数目）</p>
<p>注意：有序点云数据集，意味着点云是类似于图像（或者矩阵）的结构，数据分为行和列。这种点云的实例包括立体摄像机和时间飞行摄像机生成的数据。有序数据集的优势在于，预先了解相邻点（和像素点类似）的关系，邻域操作更加高效，这样就加速了计算并降低了PCL中某些算法的成本。</p>
<p>例如：WIDTH 640    # 每行有640个点</p>
</li>
</ul>
</li>
<li><p><strong>HEIGHT –用点的数目表示点云数据集的高度</strong>。类似于WIDTH ，HEIGHT也有两层解释：</p>
<ul>
<li><p>它表示有序点云数据集的高度（行的总数）</p>
<ul>
<li><p>有序点云例子：</p>
<p>WIDTH 640    # 像图像一样的有序结构，有640行和480列，</p>
<p>HEIGHT 480   # 这样该数据集中共有640*480=307200个点</p>
</li>
</ul>
</li>
<li><p>对于无序数据集它被设置成1（被用来检查一个数据集是有序还是无序）</p>
<ul>
<li><p>无序点云例子：</p>
<p>WIDTH 307200</p>
<p>HEIGHT 1    # 有307200个点的无序点云数据集</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>VIEWPOINT–指定数据集中点云的获取视点</strong>。VIEWPOINT有可能在不同坐标系之间转换的时候应用，在辅助获取其他特征时也比较有用，例如曲面法线，在判断方向一致性时，需要知道视点的方位，视点信息被指定为平移（txtytz）+四元数（qwqxqyqz）。默认值是：VIEWPOINT 0 0 0 1 0 0 0</p>
</li>
<li><p><strong>POINTS–指定点云中点的总数</strong>。从0.7版本开始，该字段就有点多余了，因此有可能在将来的版本中将它移除。</p>
<p>例子：POINTS 307200  #点云中点的总数为307200</p>
</li>
<li><p><strong>DATA –指定存储点云数据的数据类型</strong>。从0.7版本开始，支持两种数据类型：<strong>ascii和二进制</strong>。</p>
</li>
</ul>
<p>注意：文件头最后一行（DATA）的下一个字节就被看成是点云的数据部分了，它会被解释为点云数据。</p>
<p>警告：PCD文件的文件头部分必须以上面的顺序精确指定，也就是如下顺序：</p>
<p>VERSION、FIELDS、SIZE、TYPE、COUNT、WIDTH、HEIGHT、VIEWPOINT、POINTS、DATA，之间用换行隔开。</p>
<h3 id="数据存储类型"><a href="#数据存储类型" class="headerlink" title="数据存储类型"></a>数据存储类型</h3><p>在0.7版本中，.PCD文件格式用两种模式存储数据：</p>
<p>如果以ASCII形式，每一点占据一个新行：</p>
<p>p_1</p>
<p>p_2</p>
<p>…</p>
<p>p_n</p>
<p>注意：从PCL 1.0.1版本开始，用字符串“nan”表示NaN，此字符表示该点的值不存在或非法等。</p>
<p>如果以二进制形式，这里数据是数组（向量）pcl::PointCloud.points的一份完整拷贝，在Linux系统上，我们用mmap/munmap操作来尽可能快的读写数据，存储点云数据可以用简单的ascii形式，每点占据一行，用空格键或Tab键分开，没有其他任何字符。也可以用二进制存储格式，它既简单又快速，当然这依赖于用户应用。ascii格式允许用户打开点云文件，使用例如gunplot这样的标准软件工具更改点云文件数据，或者用sed、awk等工具来对它们进行操作。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Varrella</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://varrella.github.io/2022/04/20/%E8%AE%BA%E6%96%87-%E6%AF%AB%E7%B1%B3%E6%B3%A2%E9%9B%B7%E8%BE%BE/">https://varrella.github.io/2022/04/20/%E8%AE%BA%E6%96%87-%E6%AF%AB%E7%B1%B3%E6%B3%A2%E9%9B%B7%E8%BE%BE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture1___24Pi2uS66-M___0___.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/04/20/%E7%AC%94%E8%AE%B0-SpringBoot2/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturem-1920x1080-bg-3965109.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">SpringBoot2学习笔记</div></div></a></div><div class="next-post pull-right"><a href="/2022/04/20/%E9%9D%A2%E8%AF%95-%E5%89%91%E6%8C%87Offer/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picturefh-5120x3200-bg-dd198b3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">刷题</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8AA-Deep-Learning-based-Radar-and-Camera-Sensor-Fusion-Architecture-for-Object-Detection%E3%80%8B"><span class="toc-number">1.</span> <span class="toc-text">*《A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection》</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0"><span class="toc-number">1.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E9%9B%B7%E8%BE%BE%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.0.0.0.2.</span> <span class="toc-text">2、雷达数据预处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">1.0.0.0.3.</span> <span class="toc-text">3、网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.0.0.0.4.</span> <span class="toc-text">4、论文中的实验结果</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5%E3%80%81%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0"><span class="toc-number">1.0.0.0.5.</span> <span class="toc-text">5、论文复现</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8ACenterFusion-Center-based-Radar-and-Camera-Fusion-for-3D-Object-Detection%E3%80%8B%EF%BC%88WACV2021%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">*《CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection》（WACV2021）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0-1"><span class="toc-number">2.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E9%9B%B7%E8%BE%BE%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="toc-number">2.0.0.0.2.</span> <span class="toc-text">2、雷达数据预处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-1"><span class="toc-number">2.0.0.0.3.</span> <span class="toc-text">3、网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-1"><span class="toc-number">2.0.0.0.4.</span> <span class="toc-text">4、论文中的实验结果</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5%E3%80%81%E5%A4%8D%E7%8E%B0%E7%BB%93%E6%9E%9C"><span class="toc-number">2.0.0.0.5.</span> <span class="toc-text">5、复现结果</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8ARadar-Camera-Sensor-Fusion-for-Joint-Object-Detection-and-Distance-Estimation-in-Autonomous-Vehicles%E3%80%8B"><span class="toc-number">3.</span> <span class="toc-text">*《Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles》</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0-2"><span class="toc-number">3.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">3.0.0.0.2.</span> <span class="toc-text">2、网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">3.0.0.0.3.</span> <span class="toc-text">3、论文中的实验结果</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8ARrpn-Radar-region-proposal-network-for-object-detection-in-autonomous-vehicles%E3%80%8B"><span class="toc-number">4.</span> <span class="toc-text">《Rrpn: Radar region proposal network for object detection in autonomous vehicles》</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0-3"><span class="toc-number">4.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E5%9F%BA%E4%BA%8E%E6%AF%AB%E7%B1%B3%E6%B3%A2%E9%9B%B7%E8%BE%BE%E6%A3%80%E6%B5%8B%E7%9A%84RRPN"><span class="toc-number">4.0.0.0.2.</span> <span class="toc-text">2、基于毫米波雷达检测的RRPN</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-1"><span class="toc-number">4.0.0.0.3.</span> <span class="toc-text">3、论文中的实验结果</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%E3%80%81%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0"><span class="toc-number">4.0.0.0.4.</span> <span class="toc-text">4、论文复现</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8A2D-Car-Detection-in-Radar-Data-with-PointNets%E3%80%8B%EF%BC%882019-IEEE-Intelligent-Transportation-Systems-Conference-ITSC-%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">《2D Car Detection in Radar Data with PointNets》（2019 IEEE Intelligent Transportation Systems Conference (ITSC)）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0-4"><span class="toc-number">5.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-1"><span class="toc-number">5.0.0.0.2.</span> <span class="toc-text">2、网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-2"><span class="toc-number">5.0.0.0.3.</span> <span class="toc-text">3、论文中的实验结果</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8ARADAR-RGB-attentive-fusion-for-robust-object-detection-in-autonomous-vehicles%E3%80%8B%EF%BC%88ICIP-2020%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">《RADAR+RGB attentive fusion for robust object detection in autonomous vehicles》（ICIP 2020）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0-5"><span class="toc-number">6.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-2"><span class="toc-number">6.0.0.0.2.</span> <span class="toc-text">2、网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="toc-number">6.0.0.0.3.</span> <span class="toc-text">3、论文中的检测结果</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%E3%80%81%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0-1"><span class="toc-number">6.0.0.0.4.</span> <span class="toc-text">4、论文复现</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8ADistant-Vehicle-Detection-Using-Radar-and-Vision%E3%80%8B%EF%BC%882019-International-Conference-on-Robotics-and-Automation-ICRA-%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">《Distant Vehicle Detection Using Radar and Vision》（2019 International Conference on Robotics and Automation (ICRA)）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0-6"><span class="toc-number">7.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">7.0.0.0.2.</span> <span class="toc-text">2、数据采集和预处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-2"><span class="toc-number">7.0.0.0.3.</span> <span class="toc-text">3、网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-2"><span class="toc-number">7.0.0.0.4.</span> <span class="toc-text">4、论文中的实验结果</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%8ARadar-Camera-Fusion-via-Representation-Learning-in-Autonomous-Driving%E3%80%8B-%EF%BC%88CVPR-2021"><span class="toc-number">8.</span> <span class="toc-text">《Radar Camera Fusion via Representation Learning in Autonomous Driving》 （CVPR 2021</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%E3%80%81%E8%AE%BA%E6%96%87%E6%A6%82%E8%BF%B0-7"><span class="toc-number">8.0.0.0.1.</span> <span class="toc-text">1、论文概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%E3%80%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-3"><span class="toc-number">8.0.0.0.2.</span> <span class="toc-text">2、网络结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%E3%80%81%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C-1"><span class="toc-number">8.0.0.0.3.</span> <span class="toc-text">3、论文中的检测结果</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PCD%E7%89%88%E6%9C%AC"><span class="toc-number">9.</span> <span class="toc-text">PCD版本</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%A4%B4%E6%A0%BC%E5%BC%8F"><span class="toc-number">9.0.1.</span> <span class="toc-text">文件头格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%B1%BB%E5%9E%8B"><span class="toc-number">9.0.2.</span> <span class="toc-text">数据存储类型</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/varrella/ImgHosting/blog_picture1___24Pi2uS66-M___0___.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Varrella</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="https://varrella.github.io/">我始终相信，走过平湖烟雨，岁月山河，那些历尽劫数，尝遍百味的人，会更加生动而干净。</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">简</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>